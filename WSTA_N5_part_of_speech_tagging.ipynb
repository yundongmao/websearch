{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of Speech Tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several corpora with manual part of speech (POS) tagging are included in NLTK. For this exercise, we'll use a sample of the Penn Treebank corpus, a collection of Wall Street Journal articles. We can access the part-of-speech information for either the Penn Treebank or the Brown as follows. We use sentences here because that is the preferred representation for doing POS tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'Pierre', u'NNP'), (u'Vinken', u'NNP'), (u',', u','), (u'61', u'CD'), (u'years', u'NNS'), (u'old', u'JJ'), (u',', u','), (u'will', u'MD'), (u'join', u'VB'), (u'the', u'DT'), (u'board', u'NN'), (u'as', u'IN'), (u'a', u'DT'), (u'nonexecutive', u'JJ'), (u'director', u'NN'), (u'Nov.', u'NNP'), (u'29', u'CD'), (u'.', u'.')]\n",
      "[u'Pierre', u'Vinken', u',', u'61', u'years', u'old', u',', u'will', u'join', u'the', u'board', u'as', u'a', u'nonexecutive', u'director', u'Nov.', u'29', u'.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import treebank, brown\n",
    "\n",
    "print treebank.tagged_sents()[0]\n",
    "print treebank.sents()[0]\n",
    "# print brown.tagged_sents()[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLTK, word/tag pairs are stored as tuples, the transformation from the plain text \"word/tag\" representation to the python data types is done by the corpus reader.\n",
    "\n",
    "The two corpus do not have the same tagset; the Brown was tagged with a more fine-grained tagset: for instance, instead of \"DT\" (determiner) as in the Penn Treebank, the word \"the\" is tagged as \"AT\" (article, which is a kind of determiner). We can actually convert them both to the Universal tagset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('universal_tagset')\n",
    "print treebank.tagged_sents(tagset=\"universal\")[0]\n",
    "print brown.tagged_sents(tagset=\"universal\")[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a basic unigram POS tagger. First, we need to collect POS distributions for each word. We'll do this (somewhat inefficiently) using a dictionary of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "POS_dict = defaultdict(dict)\n",
    "for word_pos_pair in treebank.tagged_words():\n",
    "    word = word_pos_pair[0].lower()\n",
    "    POS = word_pos_pair[1]\n",
    "    POS_dict[word][POS] = POS_dict[word].get(POS,0) + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at some words which appear with multiple POS, and their POS counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "increase\n",
      "{u'VB': 11, u'NN': 20}\n",
      "refunding\n",
      "{u'VBG': 1, u'NN': 3}\n",
      "straight\n",
      "{u'RB': 2, u'JJ': 3}\n",
      "second\n",
      "{u'JJ': 16, u'NNP': 2}\n",
      "contributed\n",
      "{u'VBN': 1, u'VBD': 6}\n",
      "reported\n",
      "{u'VBN': 12, u'VBD': 25}\n",
      "elaborate\n",
      "{u'VB': 7, u'JJ': 2}\n",
      "climbed\n",
      "{u'VBN': 1, u'VBD': 4}\n",
      "reports\n",
      "{u'VBZ': 2, u'NNS': 13}\n",
      "sino-u.s.\n",
      "{u'JJ': 1, u'NNP': 1}\n",
      "criticism\n",
      "{u'NN': 1, u'NNP': 1}\n",
      "brought\n",
      "{u'VBN': 3, u'VBD': 3}\n"
     ]
    }
   ],
   "source": [
    "for word in POS_dict.keys()[:100]:\n",
    "    if len(POS_dict[word]) > 1:\n",
    "        print word\n",
    "        print POS_dict[word]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common ambiguities that we see here are between nouns and verbs (<i>increase</i>, <i>refunding</i>, <i>reports</i>), and, among verbs, between past tense and past participles (<i>contributed</i>, <i>reported</i>, <i>climbed</i>).\n",
    "\n",
    "To create an actual tagger, we just need to pick the most common tag for each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(u'The', 'NN'), (u'Fulton', 'NN'), (u'County', 'NN'), (u'Grand', 'NN'), (u'Jury', 'NN'), (u'said', u'VBD'), (u'Friday', 'NN'), (u'an', u'DT'), (u'investigation', u'NN'), (u'of', u'IN'), (u\"Atlanta's\", 'NN'), (u'recent', u'JJ'), (u'primary', u'JJ'), (u'election', u'NN'), (u'produced', u'VBN'), (u'``', u'``'), (u'no', u'DT'), (u'evidence', u'NN'), (u\"''\", u\"''\"), (u'that', u'IN'), (u'any', u'DT'), (u'irregularities', 'NN'), (u'took', u'VBD'), (u'place', u'NN'), (u'.', u'.')]\n"
     ]
    }
   ],
   "source": [
    "tagger_dict = {}\n",
    "for word in POS_dict:\n",
    "    tagger_dict[word] = max(POS_dict[word],key=lambda (x): POS_dict[word][x])\n",
    "\n",
    "def tag(sentence):\n",
    "    return [(word,tagger_dict.get(word,\"NN\")) for word in sentence]\n",
    "\n",
    "print tag(brown.sents()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though we'd probably want some better handling of capitalized phrases (backing off to NNP rather than NN when a word is capitalized), and there are a few other obvious errors, generally it's not too bad. \n",
    "\n",
    "NLTK has built-in support for n-gram taggers; Let's build unigram and bigram taggers, and test their performance. First we need to split our corpus into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdfasdf\n"
     ]
    }
   ],
   "source": [
    "size = int(len(treebank.tagged_sents()) * 0.9)\n",
    "train_sents = treebank.tagged_sents()[:size]\n",
    "test_sents = treebank.tagged_sents()[size:]\n",
    "# print \"asdfasdf\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first compare a unigram and bigram tagger. All NLTK taggers have an evaluate method which prints out the accuracy on some test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.864732824427\n",
      "[(u'The', u'DT'), (u'jury', u'NN'), (u'further', u'JJ'), (u'said', u'VBD'), (u'in', u'IN'), (u'term-end', None), (u'presentments', None), (u'that', u'IN'), (u'the', u'DT'), (u'City', u'NNP'), (u'Executive', None), (u'Committee', u'NNP'), (u',', u','), (u'which', u'WDT'), (u'had', u'VBD'), (u'over-all', None), (u'charge', u'NN'), (u'of', u'IN'), (u'the', u'DT'), (u'election', u'NN'), (u',', u','), (u'``', u'``'), (u'deserves', None), (u'the', u'DT'), (u'praise', None), (u'and', u'CC'), (u'thanks', u'NNS'), (u'of', u'IN'), (u'the', u'DT'), (u'City', u'NNP'), (u'of', u'IN'), (u'Atlanta', u'NNP'), (u\"''\", u\"''\"), (u'for', u'IN'), (u'the', u'DT'), (u'manner', u'NN'), (u'in', u'IN'), (u'which', u'WDT'), (u'the', u'DT'), (u'election', u'NN'), (u'was', u'VBD'), (u'conducted', u'VBN'), (u'.', u'.')]\n",
      "0.13465648855\n",
      "[(u'The', u'DT'), (u'jury', u'NN'), (u'further', u'RB'), (u'said', u'VBD'), (u'in', u'IN'), (u'term-end', None), (u'presentments', None), (u'that', None), (u'the', None), (u'City', None), (u'Executive', None), (u'Committee', None), (u',', None), (u'which', None), (u'had', None), (u'over-all', None), (u'charge', None), (u'of', None), (u'the', None), (u'election', None), (u',', None), (u'``', None), (u'deserves', None), (u'the', None), (u'praise', None), (u'and', None), (u'thanks', None), (u'of', None), (u'the', None), (u'City', None), (u'of', None), (u'Atlanta', None), (u\"''\", None), (u'for', None), (u'the', None), (u'manner', None), (u'in', None), (u'which', None), (u'the', None), (u'election', None), (u'was', None), (u'conducted', None), (u'.', None)]\n"
     ]
    }
   ],
   "source": [
    "from nltk import UnigramTagger, BigramTagger\n",
    "\n",
    "unigram_tagger = UnigramTagger(train_sents)\n",
    "bigram_tagger = BigramTagger(train_sents)\n",
    "print unigram_tagger.evaluate(test_sents)\n",
    "print unigram_tagger.tag(brown.sents()[1])\n",
    "print bigram_tagger.evaluate(test_sents)\n",
    "print bigram_tagger.tag(brown.sents()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unigram tagger does way better. The reason is sparsity, the bigram tagger doesn't have counts for many of the word/tag context pairs; what's worse, once it can't tag something, it fails catastrophically for the rest of the sentence tag, because it has no counts at all for missing tag contexts. We can fix this by adding backoffs, including the default tagger with just tags everything as NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.891704834606\n",
      "0.883155216285\n"
     ]
    }
   ],
   "source": [
    "from nltk import DefaultTagger\n",
    "\n",
    "default_tagger = DefaultTagger(\"NN\")\n",
    "unigram_tagger = UnigramTagger(train_sents,backoff=default_tagger)\n",
    "bigram_tagger = BigramTagger(train_sents,backoff=unigram_tagger)\n",
    "\n",
    "print bigram_tagger.evaluate(test_sents)\n",
    "print unigram_tagger.evaluate(test_sents)\n",
    "# print bigram_tagger.tag(brown.sents()[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see a 3% increase in performance from adding the bigram information on top of the unigram information.\n",
    "\n",
    "NLTK has interfaces to the Brill tagger (nltk.tag.brill) and also pre-build, state-of-the-art sequential POS tagging models, for instance the Stanford POS tagger (StanfordPOSTagger), which is what you should use if you actually need high-quality POS tagging for some application; if you are working on a computer with the Stanford CoreNLP tools installed and NLTK set up to use them (this is the case for the lab computers where workshops are held), the below code should work. If not, see the documentation <a href=\"https://github.com/nltk/nltk/wiki/Installing-Third-Party-Software\"> here </a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asfd\n"
     ]
    }
   ],
   "source": [
    "from nltk import StanfordPOSTagger\n",
    "# print 'asfd'\n",
    "stanford_tagger = StanfordPOSTagger('english-bidirectional-distsim.tagger')\n",
    "# print stanford_tagger.tag(brown.sents()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
