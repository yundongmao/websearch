{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Student Name:Mao Yundong\n",
    "\n",
    "Student ID:882542\n",
    "\n",
    "Python version used:2.7.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "import json\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def myLemmatize(lemmatizer,tWord):\n",
    "    temp = lemmatizer.lemmatize(tWord)\n",
    "    if len(temp) < len(tWord):\n",
    "        return temp\n",
    "    return lemmatizer.lemmatize(tWord,\"v\")\n",
    "# f = open(\"./assignment_group/project_files/testing.json\")\n",
    "# a = f.readline()\n",
    "# dict = json.loads(a)\n",
    "# print (len(dict))\n",
    "# print (dict[0])\n",
    "\n",
    "\n",
    "f2 = open(\"./assignment_group/project_files/documents.json\")\n",
    "a = f2.readline()\n",
    "doc_dict = json.loads(a)\n",
    "# for s in doc_dict[0][\"text\"]:\n",
    "#     print(s)\n",
    "\n",
    "\n",
    "word_numbers = {}\n",
    "num_docs = []\n",
    "num_paras = [] \n",
    "para_doc = {}\n",
    "doc_para = {}\n",
    "wordnum_freq_doc = collections.defaultdict(list)\n",
    "wordnum_freq_para = collections.defaultdict(list)\n",
    "for i,doc in enumerate(doc_dict):\n",
    "    temp_doc = []\n",
    "    temp_doc_set = set()\n",
    "    doc_para[i] = [len(num_paras)]\n",
    "    for j,para in enumerate(doc[\"text\"]):\n",
    "        temp_para_set = set()\n",
    "        temp_para = []\n",
    "        words = re.split(\",|\\.| |\\?|!\",para)\n",
    "        for word in words:\n",
    "            if word.isalpha():\n",
    "                tempword = myLemmatize(lemmatizer,word.lower())\n",
    "                if tempword not in stopWords:\n",
    "                    wi = word_numbers.setdefault(tempword,len(word_numbers))\n",
    "                    temp_para.append(wi)\n",
    "                    temp_doc_set.add(wi)\n",
    "                    temp_para_set.add(wi)\n",
    "        para_doc[len(num_paras)] = (i,j)\n",
    "        num_paras.append(temp_para)\n",
    "        temp_doc.append(temp_para)\n",
    "        for wordnum in temp_para_set:\n",
    "            wordnum_freq_para[wordnum].append((i,j))\n",
    "    doc_para[i].append(len(num_paras))\n",
    "    for wordnum in temp_doc_set:\n",
    "        wordnum_freq_doc[wordnum].append(i)\n",
    "    if i% 50 == 0:\n",
    "        print i\n",
    "    num_docs.append(temp_doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32350\n",
      "28165\n"
     ]
    }
   ],
   "source": [
    "keyword_doc_numset = set([wi for wi in wordnum_freq_doc.keys() if len(wordnum_freq_doc[wi])<=1])\n",
    "keyword_para_numset = set([wi for wi in wordnum_freq_para.keys() if len(wordnum_freq_para[wi])<=1])\n",
    "print len(keyword_doc_numset)\n",
    "print len(keyword_para_numset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441\n",
      "55893\n",
      "[u'constan\\u021ba', u'belligerence', u'cosmogenic', u'schlegel', u'saburai', u'nunnery', u'chthonic', u'sowell', u'sonja', u'panchala', u'circuitry', u'pantheistic', u'francesco', u'impermanence', u'woody', u'saburau', u'kalmar', u'cul\\xe9', u'nordisk', u'lenca', u'scold', u'praze', u'originality', u'\\u6cb3', u'dachen', u'willibrordus', u'crossbar', u'hermann', u'ioannidis', u't\\xedn', u'vazgen', u'namtso', u'mecopteran', u'stipulate', u'eugenics', u'lamido', u'appropriation', u'rawhide', u'\\u95a9\\u5357\\u8a9e', u'geilenkirchen', u'uncoordinated', u'lusthaus', u'saburahi', u'wooded', u'durocher', u'broiler', u'wooden', u'douillet', u'wednesday', u'normanby', u'salination', u'westhoff', u'stereotypical', u'highveld', u'saburahu', u'guardsmen', u'hayreddin', u'chiasso', u'thrace', u'kublai', u'inevitably', u'hanging', u'scraper', u'feasibility', u'bannister', u'tumed', u'selassie', u'alloway', u'francesca', u'sukra', u'kodak', u'prosody', u'inanimate', u'dormancy', u'y\\xe2hud', u'regierender', u'cyprus', u'paphos', u'usenet', u'metatextuality', u'virtuosi', u'dialogo', u'numeral', u'w\\u025br\\u02c8\\u0261\\u026a', u'succumb', u'wuju', u'evolutionism', u'\\u015bakra', u'crouch', u'moksha', u'sovena', u'manouel', u'ching', u'china', u'rlc', u'pericardial', u'chino', u'quintus', u'satrapies', u'engulfment']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import coo_matrix\n",
    "import collections\n",
    "import numpy as np\n",
    "row,column,data = [],[],[]\n",
    "for j,num_para in enumerate(num_paras):\n",
    "    tempm = collections.defaultdict(int)\n",
    "    for i in num_para:\n",
    "        tempm[i]+=1\n",
    "    for i in tempm.keys():\n",
    "        column.append(i)\n",
    "        data.append(tempm[i])\n",
    "    row.extend([j]*len(tempm))\n",
    "matr_doc = coo_matrix((data,(row,column)),shape=(len(num_paras),len(word_numbers)+1))\n",
    "\n",
    "print(len(doc_dict))\n",
    "print len(word_numbers)\n",
    "print word_numbers.keys()[0:100]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3097\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "0\n",
      "65\n",
      "3097\n"
     ]
    }
   ],
   "source": [
    "f3 = open(\"./assignment_group/project_files/devel.json\")\n",
    "a = f3.readline()\n",
    "f3.close()\n",
    "devel = json.loads(a)\n",
    "count_doc = 0\n",
    "count_para = 0\n",
    "print len(devel)\n",
    "for j,qa in enumerate(devel[:100]):\n",
    "    question = qa[\"question\"]\n",
    "    paraid = qa[\"answer_paragraph\"]\n",
    "    docid = qa[\"docid\"]\n",
    "    answer = qa[\"text\"]\n",
    "    words = re.split(\",|\\.| |\\?|!\",question)\n",
    "    num_question = []\n",
    "    for word in words:\n",
    "        if word.isalpha():\n",
    "            tempword = myLemmatize(lemmatizer,word.lower())\n",
    "            if tempword not in stopWords:\n",
    "                num_question.append(word_numbers.get(tempword,len(word_numbers)))\n",
    "    row,column,data = [],[],[]\n",
    "    tempm = collections.defaultdict(int)\n",
    "    for i in num_question:\n",
    "        tempm[i]+=1\n",
    "    for i in tempm.keys():\n",
    "        column.append(i)\n",
    "        data.append(tempm[i])\n",
    "    row.extend([0]*len(tempm))\n",
    "    matr = coo_matrix((data,(row,column)),shape=(1,len(word_numbers)+1))\n",
    "    cos_sims = cosine_similarity(matr,matr_doc)\n",
    "    paraindex = np.argmax(cos_sims[0])\n",
    "#     for wi in num_question:\n",
    "#         if wi in keyword_para_numset:\n",
    "#             if docid ==  wordnum_freq_para[wi][0][0]:\n",
    "#                 count_doc+=1\n",
    "#                 if paraid == wordnum_freq_para[wi][0][1]:\n",
    "#                     count_para+=1\n",
    "#             break\n",
    "#     else:\n",
    "    start = doc_para[docid][0]\n",
    "    end = doc_para[docid][1]\n",
    "    paraindex = np.argmax(cos_sims[0][start:end])\n",
    "    paraindex2 = np.argmax(np.concatenate((cos_sims[0][start:end][:paraindex],cos_sims[0][start:end][paraindex+1:])))\n",
    "#         paraindex2 = np.argmax(cos_sims[0][start:end][:paraindex]+cos_sims[0][start:end][paraindex+1:])\n",
    "    if paraid == para_doc[paraindex+start][1]:\n",
    "        count_para+=1\n",
    "    elif paraid == para_doc[paraindex2+start][1]:\n",
    "        count_para+=1\n",
    "    \n",
    "#     for wi in num_question:\n",
    "#         if wi in keyword_para_numset:\n",
    "#             if docid ==  wordnum_freq_para[wi][0][0]:\n",
    "#                 count_doc+=1\n",
    "#                 if paraid == wordnum_freq_para[wi][0][1]:\n",
    "#                     count_para+=1\n",
    "#             break\n",
    "#     else:\n",
    "#         for wi in num_question:\n",
    "#             if wi in keyword_doc_numset:\n",
    "#     #             print wordnum_freq_doc[wi][0]\n",
    "#                 start = doc_para[wordnum_freq_doc[wi][0]][0]\n",
    "#                 end = doc_para[wordnum_freq_doc[wi][0]][1]\n",
    "#                 paraindex = np.argmax(cos_sims[0][start:end])\n",
    "#                 if docid ==  para_doc[paraindex+start][0]:\n",
    "#                     count_doc+=1\n",
    "#                     if paraid == para_doc[paraindex+start][1]:\n",
    "#                         count_para+=1\n",
    "#                 break\n",
    "#         else:\n",
    "#             if docid ==  para_doc[paraindex][0]:\n",
    "#                 count_doc+=1\n",
    "#                 if paraid == para_doc[paraindex][1]:\n",
    "#                     count_para+=1\n",
    "    if j%30==0:\n",
    "        print j\n",
    "print count_doc\n",
    "print count_para\n",
    "print len(devel)\n",
    "#     print cos_sims[0][paraindex:paraindex+10]\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/mao/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/mao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "# print nltk.ne_chunk(nltk.corpus.treebank.tagged_sents()[11])\n",
    "nltk.download('punkt')\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "# print pos_tag(word_tokenize(\"John's big idea isn't all that bad.\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On what date did the companies that became the Computing-Tabulating-Recording Company get consolidated?\n",
      "[(u'On', 'IN'), (u'what', 'WP'), (u'date', 'NN'), (u'did', 'VBD'), (u'the', 'DT'), (u'companies', 'NNS'), (u'that', 'WDT'), (u'became', 'VBD'), (u'the', 'DT'), (u'Computing-Tabulating-Recording', 'NNP'), (u'Company', 'NNP'), (u'get', 'VB'), (u'consolidated', 'VBN'), (u'?', '.')]\n",
      "(S\n",
      "  On/IN\n",
      "  what/WP\n",
      "  date/NN\n",
      "  did/VBD\n",
      "  the/DT\n",
      "  companies/NNS\n",
      "  that/WDT\n",
      "  became/VBD\n",
      "  the/DT\n",
      "  Computing-Tabulating-Recording/NNP\n",
      "  Company/NNP\n",
      "  get/VB\n",
      "  consolidated/VBN\n",
      "  ?/.)\n",
      "(S\n",
      "  On/IN\n",
      "  what/WP\n",
      "  (NP date/NN)\n",
      "  did/VBD\n",
      "  (NP the/DT companies/NNS)\n",
      "  that/WDT\n",
      "  became/VBD\n",
      "  (NP the/DT Computing-Tabulating-Recording/NNP Company/NNP)\n",
      "  get/VB\n",
      "  consolidated/VBN\n",
      "  ?/.)\n",
      "Word: (u'On', 'IN')\n",
      "Word: (u'what', 'WP')\n",
      "Label: NP\n",
      "Leaves: [(u'date', 'NN')]\n",
      "Word: (u'date', 'NN')\n",
      "Word: (u'did', 'VBD')\n",
      "Label: NP\n",
      "Leaves: [(u'the', 'DT'), (u'companies', 'NNS')]\n",
      "Word: (u'the', 'DT')\n",
      "Word: (u'companies', 'NNS')\n",
      "Word: (u'that', 'WDT')\n",
      "Word: (u'became', 'VBD')\n",
      "Label: NP\n",
      "Leaves: [(u'the', 'DT'), (u'Computing-Tabulating-Recording', 'NNP'), (u'Company', 'NNP')]\n",
      "Word: (u'the', 'DT')\n",
      "Word: (u'Computing-Tabulating-Recording', 'NNP')\n",
      "Word: (u'Company', 'NNP')\n",
      "Word: (u'get', 'VB')\n",
      "Word: (u'consolidated', 'VBN')\n",
      "Word: (u'?', '.')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "f3 = open(\"./assignment_group/project_files/devel.json\")\n",
    "a = f3.readline()\n",
    "f3.close()\n",
    "devel = json.loads(a)\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "b = pos_tag(word_tokenize(devel[0][\"question\"]))\n",
    "print devel[0][\"question\"]\n",
    "print b\n",
    "print nltk.ne_chunk(b)\n",
    "root  = nltk.ne_chunk(b)\n",
    "# if type(root) is nltk.Tree:\n",
    "#     print \"------\"\n",
    "#     print root.label()\n",
    "#     print \"------\"\n",
    "#     print root.leaves()\n",
    "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
    "#TODO\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(b)\n",
    "print result\n",
    "\n",
    "def getNodes(parent):\n",
    "    for node in parent:\n",
    "#         print type(node)\n",
    "        if type(node) is nltk.Tree:\n",
    "            if node.label() == node:\n",
    "                print \"======== Sentence =========\"\n",
    "                print \"Sentence:\", \" \".join(node.leaves())\n",
    "            else:\n",
    "                print \"Label:\", node.label()\n",
    "                print \"Leaves:\", node.leaves()\n",
    "\n",
    "            getNodes(node)\n",
    "        else:\n",
    "            print \"Word:\", node\n",
    "\n",
    "getNodes(result)\n",
    "# print devel[0][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "--------------------\n",
      "100\n",
      "[-1.0200042  -0.0170572   0.8437923  -0.6334747   0.16771446  0.01210436\n",
      "  0.6011164  -0.20425966  0.08006173 -0.298958    0.4596039  -0.26121336\n",
      "  0.3767962  -0.21518481 -0.502386   -0.30373222 -0.8470299   0.22044711\n",
      "  0.91894007  0.67845774 -0.31429493 -0.02161172 -0.13037448 -0.14390011\n",
      " -0.28077683 -0.8498196  -1.1433948  -0.40242368 -0.11433637  0.0634269\n",
      " -0.04605196  0.06451741 -0.84069216 -0.8015875  -0.46236685  0.34117827\n",
      "  0.58227193  0.83762664  0.07902609 -0.33926144 -0.53996223  0.2380331\n",
      "  0.5503079   1.4636779  -0.5801192  -0.82101643 -0.6693629  -0.3873726\n",
      "  0.05685904  0.5873628   0.622391    1.6943349   0.62941754  0.7682547\n",
      "  0.6255334  -0.32473218 -0.53542215 -0.9180494   0.20096214 -1.9609258\n",
      "  0.74229115 -0.445484    0.45649076  0.40056372 -1.2828083   0.04189338\n",
      "  0.48259544 -0.13962056  0.87836176  0.6007734   0.67177737  1.3866149\n",
      "  1.359459   -1.2249749  -0.6223174   0.7207636  -0.1765246   0.48085836\n",
      " -0.06413845 -1.1754831   0.91755754 -0.6734247  -0.12252562  0.7314129\n",
      "  0.09066495 -0.5380573  -0.3749208   0.24227005  0.74342835  0.34251314\n",
      " -0.14409745 -0.98816097 -1.0223546  -0.35533416 -1.2000335  -0.01570101\n",
      "  0.19791377 -0.46668562 -0.6315347  -0.33670995]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "/anaconda2/lib/python2.7/site-packages/ipykernel_launcher.py:15: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import json\n",
    "f2 = open(\"./assignment_group/project_files/documents.json\")\n",
    "a = f2.readline()\n",
    "doc_dict = json.loads(a)\n",
    "print \"--------------------\"\n",
    "temp_corpus = [word_tokenize(para) for doc in doc_dict for para in doc[\"text\"]]\n",
    "print \"--------------------\"\n",
    "model = gensim.models.Word2Vec(temp_corpus)\n",
    "print \"--------------------\"\n",
    "model.save('document.embedding')\n",
    "new_model = gensim.models.Word2Vec.load('document.embedding')\n",
    "print \"--------------------\"\n",
    "print len(new_model[\"company\"])\n",
    "print new_model[\"company\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'The', u'Fulton', u'County', u'Grand', u'Jury', u'said', u'Friday', u'an', u'investigation', u'of', u\"Atlanta's\", u'recent', u'primary', u'election', u'produced', u'``', u'no', u'evidence', u\"''\", u'that', u'any', u'irregularities', u'took', u'place', u'.']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CategorizedTaggedCorpusReader' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-a1ba4f8cc67b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# model = gensim.models.Word2Vec(brown.sents())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# model.save('brown.embedding')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CategorizedTaggedCorpusReader' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "print brown.sents()[0]\n",
    "print brown.vocab\n",
    "# model = gensim.models.Word2Vec(brown.sents())\n",
    "# model.save('brown.embedding')\n",
    "# new_model = gensim.models.Word2Vec.load('brown.embedding')\n",
    "# print len(new_model[\"university\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: now you should do the same with the twitter_samples dataset. From now on, we will refer this dataset as the **training** tweets. Since this data is not tagged, the preprocessed corpus should be a list where each element is another list containing indices only (instead of (word, tag) tuples). A tokenised version of twitter_samples is available through the method .tokenized(), use this method to read your corpus. Besides generating the corpus, you should also **update** the vocabulary with the new words from this corpus.\n",
    "\n",
    "There are two things to keep in mind when doing this process:\n",
    "\n",
    "1) We will perform a bit more of preprocessing in this dataset, besides lowercasing. Specifically, you should replace special tokens with special symbols, as follows:\n",
    "- Username mentions are tokens that start with '@': replace these tokens with 'USER_TOKEN'\n",
    "- Hashtags are tokens that start with '#': replace these with 'HASHTAG_TOKEN'\n",
    "- Retweets are represented as the token 'RT' (or 'rt' if you lowercase first): replace these with 'RETWEET_TOKEN'\n",
    "- URLs are tokens that start with 'https://' or 'http://': replace these with 'URL_TOKEN'\n",
    "\n",
    "2) **Do not create a new vocabulary**. Instead, you should update the vocabulary built from PTB with any new words present in this corpus. These should *include* the special tokens defined above but *not* the original un-preprocessed tokens.\n",
    "\n",
    "The easiest way to do these steps is by doing 3 passes over the data: preprocess the words first, update the vocabulary and finally convert the corpus into the list format described above. However, it is possible to do all of this in one pass only.\n",
    "\n",
    "Print the first sentence from your preprocessed corpora, the index for the word 'electricity' and the index for 'HASHTAG_TOKEN'. (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11387, 182, 11388, 11389]\n",
      "1095\n",
      "11409\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import twitter_samples\n",
    "twitter_corpus = []\n",
    "tokens_list = twitter_samples.tokenized()\n",
    "# print tokens[0]\n",
    "\n",
    "for tokens in tokens_list:\n",
    "    temp = []\n",
    "    for token in tokens:\n",
    "        if token[0] ==\"@\":\n",
    "            temptoken = \"USER_TOKEN\"\n",
    "        elif token[0] == \"#\":\n",
    "            temptoken = \"HASHTAG_TOKEN\"\n",
    "        elif token == \"RT\":\n",
    "            temptoken = \"RETWEET_TOKEN\"\n",
    "        elif token[:min(len(\"https://\"),len(token))] == \"https://\":\n",
    "            temptoken = \"URL_TOKEN\"\n",
    "        elif token[:min(len(\"http://\"),len(token))] == \"http://\":\n",
    "            temptoken = \"URL_TOKEN\"\n",
    "        else:\n",
    "            temptoken = token.lower()\n",
    "        wi = word_numbers.setdefault(temptoken,len(word_numbers))\n",
    "        temp.append(wi)\n",
    "    twitter_corpus.append(temp)\n",
    "\n",
    "print twitter_corpus[0]\n",
    "print word_numbers[\"electricity\"]\n",
    "print word_numbers[\"HASHTAG_TOKEN\"]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions:</b> now we will preprocess the tagged twitter corpus used in W7 (Ritter et al.). This dataset will be referred from now on as **test** tweets. Before you do that though, you should update the tagset.\n",
    "\n",
    "You might have noticed this in the workshop but this dataset has a few extra tags, besides the PTB ones. These were added to incorporate specific phenomena that happens on Twitter:\n",
    "- \"USR\": username mentions\n",
    "- \"HT\": hashtags\n",
    "- \"RT\": retweets\n",
    "- \"URL\": URL addresses\n",
    "\n",
    "Notice that these special tags correspond to the special tokens we preprocessed before. These steps will be important in Part 3 later.\n",
    "\n",
    "There a few additional tags which are not specific to Twitter but are not present in the PTB sample:\n",
    "- \"VPP\"\n",
    "- \"TD\"\n",
    "- \"O\"\n",
    "\n",
    "You should add these new seven tags to the tagset you built when reading the PTB corpus.\n",
    "\n",
    "Another task is to add an extra type to the vocabulary: `<unk>`. This is in order to account for unknown or out-of-vocabulary words.\n",
    "\n",
    "Finally, build two \"inverted indices\" for the vocabulary and the tagset. These should be lists, where the \"i\"-th element should contain the word (or tag) corresponding to the index \"i\" in the vocabulary (or tagset).\n",
    "\n",
    "After doing these tasks, print the index for `<unk>` and the length of your resulting tagset. (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26070\n",
      "53\n"
     ]
    }
   ],
   "source": [
    "tag_numbers.setdefault(\"USR\", len(tag_numbers))\n",
    "tag_numbers.setdefault(\"HT\", len(tag_numbers))\n",
    "tag_numbers.setdefault(\"RT\", len(tag_numbers))\n",
    "tag_numbers.setdefault(\"URL\", len(tag_numbers))\n",
    "tag_numbers.setdefault(\"VPP\", len(tag_numbers))\n",
    "tag_numbers.setdefault(\"TD\", len(tag_numbers))\n",
    "tag_numbers.setdefault(\"O\", len(tag_numbers))\n",
    "\n",
    "word_numbers.setdefault(\"<unk>\", len(word_numbers))\n",
    "print word_numbers[\"<unk>\"]\n",
    "print len(tag_numbers)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: now we can read the test tweets. Store them in the same format as the PTB corpora (list of lists containing (word, tag) index tuples). Do the same preprocessing steps that you did for the training tweets (lowercasing + replace special tokens). However, **do not** update the vocabulary. Why? Because the test set should simulate a real-world scenario, where out-of-vocabulary words can appear. Instead, after preprocessing each word, you should check if that word is in the vocabulary. If yes, just replace it with its index, otherwise you should replace it with the index for the `<unk>` token. Remember: you can reuse the code from the workshop for this task. Just be mindful that in the workshop we stored words and tags in two separate lists: here you should have a single list, as in the PTB corpus you preprocessed above.\n",
    "\n",
    "When reading the POS tags for the test tweets you should do some additional preprocessing. There are three tags in this dataset which correspond to PTB tags but are represented with different names:\n",
    "- \"(\". In PTB, this is represented as \"-LRB-\"\n",
    "- \")\". In PTB, this is represented as \"-RRB-\"\n",
    "- \"NONE\". In PTB, this is represented as \"-NONE-\"\n",
    "\n",
    "As you build the corpus for the test tweets, you should check if the tag for a word is one of the above. If yes, you should use the PTB equivalent instead. In practice, it is sufficient to ensure you use the correct index for the corresponding tag, using your tagset dictionary. This concept is sometimes referred as *tag harmonisation*, where two different tagsets are mapped to each other.\n",
    "\n",
    "After this, print the first sentence of your preprocessed corpus. (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(11392, 46), (61, 19), (114, 11), (8, 7), (3224, 8), (170, 9), (325, 33), (1325, 19), (2375, 22), (3205, 12), (182, 9), (799, 2), (1522, 3), (16, 10), (8490, 0), (1146, 0), (2495, 0), (14038, 43), (26070, 0), (16, 10), (4263, 17), (1760, 4), (9464, 8), (2259, 17), (888, 4), (741, 8), (16, 10)]\n"
     ]
    }
   ],
   "source": [
    "import urllib\n",
    "try:\n",
    "    urllib.request.urlretrieve(\"https://github.com/aritter/twitter_nlp/raw/master/data/annotated/pos.txt\",\"pos.txt\")\n",
    "except: # Python 2\n",
    "    urllib.urlretrieve(\"https://github.com/aritter/twitter_nlp/raw/master/data/annotated/pos.txt\",\"pos.txt\")\n",
    "\n",
    "tweet_corpus = []\n",
    "with open('pos.txt') as f:\n",
    "    temps = set([\"USR\",\"RT\",\"HT\",\"URL\"])\n",
    "    tupList = []\n",
    "    for line in f:\n",
    "        if line.strip() == '':\n",
    "            tweet_corpus.append(tupList)\n",
    "            tupList = []\n",
    "        else:\n",
    "            word, pos = line.strip().split()\n",
    "#             if word[0] ==\"@\":\n",
    "#                 word = \"USER_TOKEN\"\n",
    "#             elif word[0] == \"#\":\n",
    "#                 word = \"HASHTAG_TOKEN\"\n",
    "#             elif word == \"RT\":\n",
    "#                 word = \"RETWEET_TOKEN\"\n",
    "#             elif word[:min(len(\"https://\"),len(token))] == \"https://\":\n",
    "#                 word = \"URL_TOKEN\"\n",
    "#             elif word[:min(len(\"http://\"),len(token))] == \"http://\":\n",
    "#                 word = \"URL_TOKEN\"\n",
    "#             else:\n",
    "#                 word = word.lower() \n",
    "            \n",
    "            if pos == \"USR\":\n",
    "                word = \"USER_TOKEN\"\n",
    "            elif pos == \"HT\":\n",
    "                word = \"HASHTAG_TOKEN\"\n",
    "            elif pos == \"RT\":\n",
    "                word = \"RETWEET_TOKEN\"\n",
    "            elif pos == \"URL\":\n",
    "                word = \"URL_TOKEN\"\n",
    "            else:\n",
    "                word = word.lower()\n",
    "            \n",
    "            if pos == \"(\":\n",
    "                pos = \"-LRB-\"\n",
    "            elif pos == \")\":\n",
    "                pos = \"-RRB-\"\n",
    "            elif pos == \"NONE\":\n",
    "                pos = \"-NONE-\"\n",
    "\n",
    "            if word in word_numbers:\n",
    "                tupList.append((word_numbers[word],tag_numbers[pos]))\n",
    "            else:\n",
    "                tupList.append((word_numbers[\"<unk>\"],tag_numbers[pos]))\n",
    "\n",
    "print tweet_corpus[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Hint</b>: if you did these steps correctly you should have 53 tags in your tagset and around 26000 words in your vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Running the PTB tagger on the test tweets (1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: your next task is to train a POS tagger on the PTB data and try it on the test tweets. This is exactly what we did in W7: feel free to reuse code. However, we are also gonna modify the code a bit.\n",
    "\n",
    "Your first task is encapsulate the HMM training code into a function. You should name your function `count`. This function should take these input parameters:\n",
    "- A tagged corpus, in the format described above (list of lists containing (word, tag) index tuples).\n",
    "- The vocabulary (a dict).\n",
    "- The tagset (a dict).\n",
    "\n",
    "Output return values should contain:\n",
    "- The initial tag probabilities (a vector).\n",
    "- The transition probabilities (a matrix).\n",
    "- The emission probabilities (a matrix).\n",
    "\n",
    "Notice that in the workshop code the vocabulary and tagset were built as part of the training process. Here you should pass them explicitly as parameters instead. This is to ensure our tagger can take into account the words in the training tweets and the extra tags. Important: the workshop code initialise the probabilities with an `eps` value, to ensure you end up with non-zero probabilities for unseen events. You should do the same here.\n",
    "\n",
    "After writing your function, run it on the PTB corpus to obtain the initial, transition and emission probabilities. (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def count(tagged_tup_corpus,vocabulary,tagest): \n",
    "    S = len(tag_numbers)\n",
    "    V = len(word_numbers)\n",
    "\n",
    "    # initalise\n",
    "    eps = 0.1\n",
    "    pi = eps * np.ones(S)\n",
    "    A = eps * np.ones((S, S))\n",
    "    O = eps * np.ones((S, V))\n",
    "\n",
    "    # count\n",
    "    for sent in tagged_tup_corpus:\n",
    "        last_tag = None\n",
    "        for word, tag in sent:\n",
    "            O[tag, word] += 1\n",
    "            if last_tag == None:\n",
    "                pi[tag] += 1\n",
    "            else:\n",
    "                A[last_tag, tag] += 1\n",
    "            last_tag = tag\n",
    "        \n",
    "    # normalise\n",
    "    pi /= np.sum(pi)\n",
    "    for s in range(S):\n",
    "        O[s,:] /= np.sum(O[s,:])\n",
    "        A[s,:] /= np.sum(A[s,:])\n",
    "    return pi,A,O\n",
    "\n",
    "\n",
    "\n",
    "params = count(num_corpus,word_numbers,tag_numbers)\n",
    "# print params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: now you should write a function for Viterbi. The input parameters are the same as in the workshop:\n",
    "- The parameters (probabilities) of your HMM (a tuple (initial, transition, emission)).\n",
    "- The input words (a list with numbers).\n",
    "\n",
    "The output is slightly different though:\n",
    "- A list of (word, tag) indices, containing the original input word and the predicted tag.\n",
    "\n",
    "Run Viterbi on the test tweets and store the predictions in a list (might take a few seconds). Remember that in the processing part you stored the test tweets as (word, tag) indices lists: make sure your input to Viterbi are word index lists only. Print the first sentence of your predicted list. (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(11392, 27), (61, 19), (114, 11), (8, 7), (3224, 8), (170, 9), (325, 33), (1325, 19), (2375, 22), (3205, 12), (182, 9), (799, 2), (1522, 3), (16, 10), (8490, 29), (1146, 8), (2495, 8), (14038, 10), (26070, 38), (16, 10), (4263, 29), (1760, 4), (9464, 8), (2259, 17), (888, 4), (741, 8), (16, 10)]\n"
     ]
    }
   ],
   "source": [
    "# def viterbi(params, observations):\n",
    "#     pi, A, O = params\n",
    "#     M = len(observations)\n",
    "#     S = pi.shape[0]\n",
    "    \n",
    "#     alpha = np.zeros((M, S))\n",
    "#     alpha[:,:] = float('-inf')\n",
    "#     backpointers = np.zeros((M, S), 'int')\n",
    "    \n",
    "#     # base case\n",
    "#     alpha[0, :] = pi * O[:,observations[0]]\n",
    "    \n",
    "#     # recursive case\n",
    "#     for t in range(1, M):\n",
    "#         for s2 in range(S):\n",
    "#             for s1 in range(S):\n",
    "#                 score = alpha[t-1, s1] * A[s1, s2] * O[s2, observations[t]]\n",
    "#                 if score > alpha[t, s2]:\n",
    "#                     alpha[t, s2] = score\n",
    "#                     backpointers[t, s2] = s1\n",
    "    \n",
    "#     # now follow backpointers to resolve the state sequence\n",
    "#     ss = []\n",
    "#     ss.append((observations[M-1],np.argmax(alpha[M-1,:])))\n",
    "#     for i in range(M-1, 0, -1):\n",
    "#         ss.append((observations[i-1],backpointers[i, ss[-1][1]]))\n",
    "        \n",
    "#     return list(reversed(ss))\n",
    "\n",
    "\n",
    "\n",
    "def viterbi(params, observations):\n",
    "    pi, A, O = params\n",
    "    M = len(observations)\n",
    "    S = pi.shape[0]\n",
    "    \n",
    "    alpha = np.zeros((M, S))\n",
    "    alpha[:,:] = float('-inf')\n",
    "    backpointers = np.zeros((M, S), 'int')\n",
    "    \n",
    "    # base case\n",
    "    alpha[0, :] = pi * O[:,observations[0]]\n",
    "    \n",
    "    # recursive case\n",
    "    for t in range(1, M):\n",
    "        temp = alpha[t-1] * A.T * O.T[observations[t]].reshape(S,1)\n",
    "        for s2 in range(S):\n",
    "            backpointers[t, s2] = np.argmax(temp[s2])\n",
    "            alpha[t, s2] = temp[s2][backpointers[t, s2]]\n",
    "\n",
    "    ss = []\n",
    "    ss.append((observations[M-1],np.argmax(alpha[M-1,:])))\n",
    "    for i in range(M-1, 0, -1):\n",
    "        ss.append((observations[i-1],backpointers[i, ss[-1][1]]))\n",
    "    return list(reversed(ss))\n",
    "\n",
    "\n",
    "\n",
    "# predictionTupList = []\n",
    "# for index,tupList in enumerate(tweet_corpus):\n",
    "#     observations = [tup[0] for tup in tupList]\n",
    "#     predictionTupList.append(viterbi(params, observations))\n",
    "# #     if index == 10:\n",
    "# #         break\n",
    "#     print index\n",
    "\n",
    "predictionTupList = []\n",
    "for tupList in tweet_corpus:\n",
    "    observations = [tup[0] for tup in tupList]\n",
    "    predictionTupList.append(viterbi(params, observations))\n",
    "print predictionTupList[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: you should now evaluate the results. Write a function that takes (word, tag) lists as inputs and outputs the tag sequence using the original tags in the tagset. Your inputs should be a sentence and the tag inverted index you built before.\n",
    "\n",
    "Run this function on the predictions you obtained above **and** the test tweets, storing them in two separate lists. Finally, flat your predictions into a single list and do the same for the test tweets and report accuracy. (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6374053342113928\n"
     ]
    }
   ],
   "source": [
    "def getTagSequenceList(tupSeqList):\n",
    "    tagSeqList = []\n",
    "    for tupSeq in tupSeqList:\n",
    "        tagSeqList.append([tup[1] for tup in tupSeq])   \n",
    "    return tagSeqList\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score as acc\n",
    "\n",
    "all_test_tags = [tag for tags in getTagSequenceList(tweet_corpus) for tag in tags]\n",
    "all_pred_tags = [tag for tags in getTagSequenceList(predictionTupList) for tag in tags]\n",
    "\n",
    "print acc(all_test_tags, all_pred_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Adapting the tagger using prior information (1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: now your task is to adapt the tagger using prior information. What do we mean by that? Remember from part 1 that the twitter tagset has some extra tags, related to special tokens such as mentions and hashtags. In other words, **we know beforehand** that these special tokens **should** have these tags. However, because these tags never appear in the PTB data, the tagger has no such information. We are going to add this in order to improve the tagger.\n",
    "\n",
    "To recap, we know these things about the twitter data:\n",
    "- username mentions should have the tag 'USR'\n",
    "- hashtags should have the tag 'HT'\n",
    "- retweet tokens should have the tag 'RT'\n",
    "- URL tokens should have the tag 'URL'\n",
    "\n",
    "Remember how we replace these tokens with unique special ones (such as 'USER_TOKEN')? Your task is to adapt the emission probabilities for these tokens. Modify the emission matrix: assign 1.0 probability for the emission P('USER_TOKEN'|'USR') and 0.0 for P(word|'USR') for all other words. Do the same for the other three special tags.\n",
    "\n",
    "In order to do that, you should use the vocabulary and tagset dictionaries in order to obtain the indices for the corresponding words and tags. Then, use the indices to find the values in the emission matrix and modify them. Print your new emission matrix. (0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[9.15362275e-05 1.74750980e-04 8.32147523e-06 ... 8.32147523e-06\n",
      "  8.32147523e-06 8.32147523e-06]\n",
      " [1.33456113e-05 1.33456113e-05 6.51946457e-01 ... 1.33456113e-05\n",
      "  1.33456113e-05 1.33456113e-05]\n",
      " [1.62519706e-05 1.62519706e-05 1.62519706e-05 ... 1.62519706e-05\n",
      "  1.62519706e-05 1.62519706e-05]\n",
      " ...\n",
      " [3.83567949e-05 3.83567949e-05 3.83567949e-05 ... 3.83567949e-05\n",
      "  3.83567949e-05 3.83567949e-05]\n",
      " [3.83567949e-05 3.83567949e-05 3.83567949e-05 ... 3.83567949e-05\n",
      "  3.83567949e-05 3.83567949e-05]\n",
      " [3.83567949e-05 3.83567949e-05 3.83567949e-05 ... 3.83567949e-05\n",
      "  3.83567949e-05 3.83567949e-05]]\n"
     ]
    }
   ],
   "source": [
    "O = params[2]\n",
    "O[tag_numbers[\"USR\"],:] = 0.0\n",
    "O[tag_numbers[\"USR\"],word_numbers[\"USER_TOKEN\"]] = 1.0\n",
    "O[tag_numbers[\"RT\"],:] = 0.0\n",
    "O[tag_numbers[\"RT\"],word_numbers[\"RETWEET_TOKEN\"]] = 1.0\n",
    "O[tag_numbers[\"URL\"],:] = 0.0\n",
    "O[tag_numbers[\"URL\"],word_numbers[\"URL_TOKEN\"]] = 1.0\n",
    "O[tag_numbers[\"HT\"],:] = 0.0\n",
    "O[tag_numbers[\"HT\"],word_numbers[\"HASHTAG_TOKEN\"]] = 1.0\n",
    "print O\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: now evaluate your new tagger on the test tweets again. You should report accuracy but also do a fine-grained error analysis. Print the F-scores for **each tag**. <b>Hint:</b> use the \"classification_report\" function in scikit-learn for that. You should report the tags that performed the best and the worse. (0.5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6967402041488311\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          #       0.00      0.00      0.00         0\n",
      "          $       0.00      0.00      0.00         0\n",
      "         ''       0.03      0.20      0.06        91\n",
      "          ,       0.85      1.00      0.92       303\n",
      "      -LRB-       0.00      0.00      0.00        32\n",
      "     -NONE-       0.00      0.00      0.00         2\n",
      "      -RRB-       0.05      0.15      0.07        34\n",
      "          .       0.72      0.83      0.77       875\n",
      "          :       0.97      0.76      0.85       562\n",
      "         CC       0.96      0.88      0.92       305\n",
      "         CD       0.59      0.59      0.59       268\n",
      "         DT       0.74      0.93      0.82       825\n",
      "         EX       0.38      0.80      0.52        10\n",
      "         FW       0.00      0.00      0.00         3\n",
      "         HT       1.00      0.99      0.99       135\n",
      "         IN       0.82      0.88      0.85      1091\n",
      "         JJ       0.63      0.59      0.61       670\n",
      "        JJR       0.49      0.74      0.59        31\n",
      "        JJS       0.88      0.81      0.84        26\n",
      "         LS       0.00      0.00      0.00         1\n",
      "         MD       0.53      0.97      0.69       181\n",
      "         NN       0.79      0.63      0.70      1931\n",
      "        NNP       0.60      0.27      0.37      1159\n",
      "       NNPS       0.00      0.00      0.00         8\n",
      "        NNS       0.44      0.54      0.48       393\n",
      "          O       0.00      0.00      0.00         1\n",
      "        PDT       0.00      0.00      0.00         1\n",
      "        POS       0.39      0.78      0.52        36\n",
      "        PRP       0.86      0.82      0.84      1106\n",
      "       PRP$       0.84      0.86      0.85       234\n",
      "         RB       0.71      0.76      0.73       680\n",
      "        RBR       0.62      0.25      0.36        20\n",
      "        RBS       0.07      0.33      0.12         3\n",
      "         RP       0.64      0.45      0.53       110\n",
      "         RT       1.00      1.00      1.00       152\n",
      "        SYM       0.00      0.00      0.00        13\n",
      "         TD       0.00      0.00      0.00         1\n",
      "         TO       0.84      0.96      0.89       264\n",
      "         UH       1.00      0.00      0.00       493\n",
      "        URL       1.00      0.99      1.00       183\n",
      "        USR       1.00      0.97      0.98       464\n",
      "         VB       0.65      0.70      0.68       660\n",
      "        VBD       0.77      0.74      0.75       306\n",
      "        VBG       0.89      0.50      0.64       303\n",
      "        VBN       0.43      0.63      0.51       140\n",
      "        VBP       0.78      0.64      0.70       527\n",
      "        VBZ       0.69      0.78      0.73       342\n",
      "        VPP       0.00      0.00      0.00         1\n",
      "        WDT       0.38      0.47      0.42        19\n",
      "         WP       0.97      0.74      0.84        47\n",
      "        WP$       0.00      0.00      0.00         0\n",
      "        WRB       1.00      0.81      0.90       143\n",
      "         ``       0.00      0.00      0.00         0\n",
      "\n",
      "avg / total       0.76      0.70      0.70     15185\n",
      "\n",
      "NNP is the worst\n",
      "USR is the best\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predictionTupList = []\n",
    "# print len(tweet_corpus)\n",
    "for tupList in tweet_corpus:\n",
    "    observations = [tup[0] for tup in tupList]\n",
    "    predictionTupList.append(viterbi(params, observations))\n",
    "all_test_tags = [tag for tags in getTagSequenceList(tweet_corpus) for tag in tags]\n",
    "all_pred_tags = [tag for tags in getTagSequenceList(predictionTupList) for tag in tags]\n",
    "\n",
    "tag_name = {}\n",
    "for key in tag_numbers.keys():\n",
    "    tag_name[tag_numbers[key]] = key\n",
    "print acc(all_test_tags, all_pred_tags)\n",
    "all_test_tags_name = [tag_name[c] for c in all_test_tags]\n",
    "all_pred_tags_name = [tag_name[c] for c in all_pred_tags]\n",
    "print classification_report(all_test_tags_name,all_pred_tags_name)\n",
    "\n",
    "print \"NNP is the worst\"\n",
    "print \"USR is the best\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: finally, based on the information you got above, do some analysis. Why do you think the tagger performed worse on the tags you mentioned above? How would you improve the tagger? Feel free to inspect some instances manually if you want (and show us if you do). Write your analysis in the markdown cell below. Notice that this question is inherently subjective: this is on purpose as you will be evaluated on your analytical abilities. But don't worry about going into depth: 2-4 sentences is enough (but feel free to write more if you need). (0.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>WRITE YOUR ANALYSIS HERE</b>\n",
    "###my analysis\n",
    "    Proper nouns (NNP) name specific people, places, things, or ideas. Since they these nouns are naming specific things, they always begin with a capital letter. But in our process word will be remain as lower. So lost some information. I think if we can keep the lower or upper character when it doesn't appear in the head. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra credits 1: Expectation-Maximisation (1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: here your goal is to improve the tagger using **hard EM**. This question is divided in two parts. Because EM can take a long time to run we will modify our code above to make it faster and also more robust to underflow by making calculations in the log space.\n",
    "\n",
    "Your first task is to modify the `count` and `viterbi` functions. For `count`, you should return log probabilities for all matrices. For `viterbi`, you should modify the code in the following way:\n",
    "- Calculate scores using log probabilities. Remember that in log space, any products become sums. Also remember to make sure you change the base case as well (not only the inner loop).\n",
    "- You should rewrite the algorithm in vectorised form, in order to make it more efficient. Remember that in Viterbi, the third (inner) `for` loop calculate scores for a single cell, while the second `for` loop calculates scores for a whole column. These operations can be made in parallel, which makes them amenable to vectorisation. Replace the second and third `for` loops in the code with appropriate vector and matrix operations. Remember to do the same to obtain the backpointers. <b>Hint</b>: start by vectorising the inner loop only and check if it is correct.\n",
    "\n",
    "The second task is to implement EM. This can be done without the first step above but bear in mind that it will take much longer to run. Write a function that:\n",
    "\n",
    "- 1) Tag a corpus using Viterbi and an initial tagger\n",
    "- 2) Train a new tagger using the tagged corpora obtained above.\n",
    "- 3) Repeat both steps above N times.\n",
    "\n",
    "If you've done the main homework correctly you should be able to reuse the `count` and `viterbi` functions for this and the code should be very straightforward. You should pretrain your tagger using the steps in the main homework, including the tag adaptation in Part 3. Then, in the inner loop, use the training tweets as your unlabelled corpora. Run 5 iterations of EM and report test accuracy **at every iteration**.\n",
    "\n",
    "EM can take a long time to train, even if you're using the vectorised Viterbi code. If it is too slow in your machine, you're allowed to use a subset of the training tweets for this task.\n",
    "\n",
    "To get full marks, adapt the algorithm in the following manner:\n",
    "- At step 2) above, when training the new tagger, combine **both** the PTB gold data and the tagged training tweets, instead of just using the training tweets.\n",
    "\n",
    "This is an easy trick to obtain better results (and it is essentially **semi-supervised** learning)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------version 1--------\n",
      "0.699703654922621\n",
      "0.6987816924596641\n",
      "0.6979255844583471\n",
      "0.6966743496871913\n",
      "0.6964767863022719\n",
      "----------version 2----------\n",
      "I use the last params and tag together to update the new params\n",
      "0.7183404675666777\n",
      "0.7218307540335858\n",
      "0.7237405334211393\n",
      "0.7234112611129404\n",
      "0.7230819888047415\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import collections\n",
    "\n",
    "def viterbi(params, observations):\n",
    "    pi, A, O = params\n",
    "    M = len(observations)\n",
    "    S = pi.shape[0]\n",
    "    \n",
    "    alpha = np.zeros((M, S))\n",
    "    alpha[:,:] = float('-inf')\n",
    "    backpointers = np.zeros((M, S), 'int')\n",
    "    \n",
    "    # base case\n",
    "    alpha[0, :] = pi + O[:,observations[0]]\n",
    "    \n",
    "    # recursive case\n",
    "    for t in range(1, M):\n",
    "        temp = alpha[t-1] + A.T + O.T[observations[t]].reshape(S,1)\n",
    "#         TODO\n",
    "        for s2 in range(S):\n",
    "            backpointers[t, s2] = np.argmax(temp[s2])\n",
    "            alpha[t, s2] = temp[s2][backpointers[t, s2]]\n",
    "\n",
    "    ss = []\n",
    "    ss.append((observations[M-1],np.argmax(alpha[M-1,:])))\n",
    "    for i in range(M-1, 0, -1):\n",
    "        ss.append((observations[i-1],backpointers[i, ss[-1][1]]))\n",
    "    return list(reversed(ss))\n",
    "def count2(tagged_tup_corpus,vocabulary,tagset,O):\n",
    "    S = len(tagset)\n",
    "    V = len(vocabulary)\n",
    "\n",
    "    # initalise\n",
    "    eps = 0.1\n",
    "    pi = eps * np.ones(S)\n",
    "    A = eps * np.ones((S, S))\n",
    "#     O = eps * np.ones((S, V))\n",
    "    O = np.e**O\n",
    "    \n",
    "    PT = eps * np.ones(S)\n",
    "    PW = eps * np.ones(V)\n",
    "    PWT = eps * np.ones((V, S))\n",
    "    CT = eps * np.ones(S)\n",
    "    CTW = eps * np.ones((S, V))\n",
    "    \n",
    "    # count\n",
    "    total = sum([len(sent) for sent in tagged_tup_corpus])\n",
    "    for sent in tagged_tup_corpus:\n",
    "        last_tag = None\n",
    "        for word, tag in sent:\n",
    "            PWT[word,tag] +=1\n",
    "            PT[tag]+=1\n",
    "            if last_tag == None:\n",
    "                pi[tag] += 1\n",
    "            else:\n",
    "                A[last_tag, tag] += 1\n",
    "            last_tag = tag\n",
    "    \n",
    "    PT /=np.sum(PT)\n",
    "    for j in range(len(vocabulary)):\n",
    "        PW[j] = np.sum(PT*O[:,j])\n",
    "#     print O[:,10]\n",
    "    PWT /= total\n",
    "    \n",
    "    #calculate p(t|w)\n",
    "    for i in range(len(vocabulary)):\n",
    "        PWT[i] /= PW[i]\n",
    "    \n",
    "    for sent in tagged_tup_corpus:\n",
    "        last_tag = None\n",
    "        for word, tag in sent:\n",
    "            CT[tag]+=PWT[word,tag]\n",
    "            CTW[tag,word]+=PWT[word,tag]\n",
    "    for s in range(S):\n",
    "        O[s] = CTW[s]/CT[s]\n",
    "    \n",
    "    \n",
    "    # normalise\n",
    "    pi /= np.sum(pi)\n",
    "    for s in range(S):\n",
    "        O[s,:] /= np.sum(O[s,:])\n",
    "        A[s,:] /= np.sum(A[s,:])\n",
    "    pi = np.log(pi)\n",
    "    O = np.log(O)\n",
    "    A = np.log(A)\n",
    "    return pi,A,O\n",
    "\n",
    "def count(tagged_tup_corpus,vocabulary,tagset): \n",
    "    S = len(tagset)\n",
    "    V = len(vocabulary)\n",
    "\n",
    "    # initalise\n",
    "    eps = 0.1\n",
    "    pi = eps * np.ones(S)\n",
    "    A = eps * np.ones((S, S))\n",
    "    O = eps * np.ones((S, V))\n",
    "\n",
    "    # count\n",
    "    for sent in tagged_tup_corpus:\n",
    "        last_tag = None\n",
    "        for word, tag in sent:\n",
    "            O[tag, word] += 1\n",
    "            if last_tag == None:\n",
    "                pi[tag] += 1\n",
    "            else:\n",
    "                A[last_tag, tag] += 1\n",
    "            last_tag = tag\n",
    "        \n",
    "    # normalise\n",
    "    pi /= np.sum(pi)\n",
    "    for s in range(S):\n",
    "        O[s,:] /= np.sum(O[s,:])\n",
    "        A[s,:] /= np.sum(A[s,:])\n",
    "    pi = np.log(pi)\n",
    "    O = np.log(O)\n",
    "    A = np.log(A)\n",
    "    return pi,A,O\n",
    "params = count(num_corpus,word_numbers,tag_numbers)\n",
    "\n",
    "O = params[2]\n",
    "O[tag_numbers[\"USR\"],:] = float('-inf')\n",
    "O[tag_numbers[\"USR\"],word_numbers[\"USER_TOKEN\"]] = 0\n",
    "O[tag_numbers[\"RT\"],:] = float('-inf')\n",
    "O[tag_numbers[\"RT\"],word_numbers[\"RETWEET_TOKEN\"]] = 0\n",
    "O[tag_numbers[\"URL\"],:] = float('-inf')\n",
    "O[tag_numbers[\"URL\"],word_numbers[\"URL_TOKEN\"]] = 0\n",
    "O[tag_numbers[\"HT\"],:] = float('-inf')\n",
    "O[tag_numbers[\"HT\"],word_numbers[\"HASHTAG_TOKEN\"]] = 0\n",
    "\n",
    "predictionTupList = []\n",
    "for tupList in tweet_corpus:\n",
    "    observations = [tup[0] for tup in tupList]\n",
    "    predictionTupList.append(viterbi(params, observations))\n",
    "all_test_tags = [tag for tags in getTagSequenceList(tweet_corpus) for tag in tags]\n",
    "    # for predictions, we need to obtain the original tag from the index\n",
    "all_pred_tags = [tag for tags in getTagSequenceList(predictionTupList) for tag in tags]\n",
    "\n",
    "# print acc(all_test_tags, all_pred_tags)\n",
    "print \"---------version 1--------\"\n",
    "for i in range(5):\n",
    "#     params = count2(predictionTupList+num_corpus,word_numbers,tag_numbers,params[2])\n",
    "    params = count(predictionTupList+num_corpus,word_numbers,tag_numbers)\n",
    "    predictionTupList = []\n",
    "    O = params[2]\n",
    "    O[tag_numbers[\"USR\"],:] = float('-inf')\n",
    "    O[tag_numbers[\"USR\"],word_numbers[\"USER_TOKEN\"]] = 0\n",
    "    O[tag_numbers[\"RT\"],:] = float('-inf')\n",
    "    O[tag_numbers[\"RT\"],word_numbers[\"RETWEET_TOKEN\"]] = 0\n",
    "    O[tag_numbers[\"URL\"],:] = float('-inf')\n",
    "    O[tag_numbers[\"URL\"],word_numbers[\"URL_TOKEN\"]] = 0\n",
    "    O[tag_numbers[\"HT\"],:] = float('-inf')\n",
    "    O[tag_numbers[\"HT\"],word_numbers[\"HASHTAG_TOKEN\"]] = 0\n",
    "    for tupList in tweet_corpus:\n",
    "        observations = [tup[0] for tup in tupList]\n",
    "        predictionTupList.append(viterbi(params, observations))\n",
    "#     all_test_tags = [tag for tags in getTagSequenceList(tweet_corpus) for tag in tags]\n",
    "    # for predictions, we need to obtain the original tag from the index\n",
    "    all_pred_tags = [tag for tags in getTagSequenceList(predictionTupList) for tag in tags]\n",
    "\n",
    "    print acc(all_test_tags, all_pred_tags)\n",
    "\n",
    "print \"----------version 2----------\"\n",
    "print \"I use the last params and tag together to update the new params\"\n",
    "params = count(num_corpus,word_numbers,tag_numbers)\n",
    "\n",
    "O = params[2]\n",
    "O[tag_numbers[\"USR\"],:] = float('-inf')\n",
    "O[tag_numbers[\"USR\"],word_numbers[\"USER_TOKEN\"]] = 0\n",
    "O[tag_numbers[\"RT\"],:] = float('-inf')\n",
    "O[tag_numbers[\"RT\"],word_numbers[\"RETWEET_TOKEN\"]] = 0\n",
    "O[tag_numbers[\"URL\"],:] = float('-inf')\n",
    "O[tag_numbers[\"URL\"],word_numbers[\"URL_TOKEN\"]] = 0\n",
    "O[tag_numbers[\"HT\"],:] = float('-inf')\n",
    "O[tag_numbers[\"HT\"],word_numbers[\"HASHTAG_TOKEN\"]] = 0\n",
    "\n",
    "predictionTupList = []\n",
    "for tupList in tweet_corpus:\n",
    "    observations = [tup[0] for tup in tupList]\n",
    "    predictionTupList.append(viterbi(params, observations))\n",
    "all_test_tags = [tag for tags in getTagSequenceList(tweet_corpus) for tag in tags]\n",
    "    # for predictions, we need to obtain the original tag from the index\n",
    "all_pred_tags = [tag for tags in getTagSequenceList(predictionTupList) for tag in tags]\n",
    "\n",
    "for i in range(5):\n",
    "    params = count2(predictionTupList+num_corpus,word_numbers,tag_numbers,params[2])\n",
    "    predictionTupList = []\n",
    "    O = params[2]\n",
    "    O[tag_numbers[\"USR\"],:] = float('-inf')\n",
    "    O[tag_numbers[\"USR\"],word_numbers[\"USER_TOKEN\"]] = 0\n",
    "    O[tag_numbers[\"RT\"],:] = float('-inf')\n",
    "    O[tag_numbers[\"RT\"],word_numbers[\"RETWEET_TOKEN\"]] = 0\n",
    "    O[tag_numbers[\"URL\"],:] = float('-inf')\n",
    "    O[tag_numbers[\"URL\"],word_numbers[\"URL_TOKEN\"]] = 0\n",
    "    O[tag_numbers[\"HT\"],:] = float('-inf')\n",
    "    O[tag_numbers[\"HT\"],word_numbers[\"HASHTAG_TOKEN\"]] = 0\n",
    "    for tupList in tweet_corpus:\n",
    "        observations = [tup[0] for tup in tupList]\n",
    "        predictionTupList.append(viterbi(params, observations))\n",
    "#     all_test_tags = [tag for tags in getTagSequenceList(tweet_corpus) for tag in tags]\n",
    "    # for predictions, we need to obtain the original tag from the index\n",
    "    all_pred_tags = [tag for tags in getTagSequenceList(predictionTupList) for tag in tags]\n",
    "\n",
    "    print acc(all_test_tags, all_pred_tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra credits 2: Soft EM using Forward-Backward (1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Instructions</b>: This is only for the truly intrepid: expect a substantial amount of work to get full marks in this. The goal is to perform **soft EM** using the Forward-Backward algorithm and expected counts. You will need to implement and adapt a set of functions for this task:\n",
    "- The `count` method, which will still be used for pretraining, needs to also calculate **final** probabilities and return these as part of the output. These are never used in Viterbi but are essential for Forward-Backward.\n",
    "- Implement the `forward` function. If you want to work in log space you should be careful because it requires summing probabilities. Hint: check the Scipy function `logsumexp` for that. Remember: the algorithm is very similar to Viterbi so feel free to use it as a starting point. The function should return the matrix with the alpha values and the marginal probability for the sentence.\n",
    "- Implement the `backward` function. This should return a matrix with the beta values and the marginal. Remember the very useful sanity check: for the same sentence and tagger, the marginals returned from `forward` and `backward` should match.\n",
    "- Implement the `expected_count` function, which is similar to `count` in the sense that it trains a tagger and output new parameters. However, it should have the alpha and beta matrices as inputs, as well as the marginals.\n",
    "\n",
    "After implementing all of this, rerun EM using the soft approach and evaluate it in a similar way as done in Extra credits 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7072769180111953\n",
      "0.7077378992426737\n",
      "0.7078037537043135\n",
      "0.7078037537043135\n",
      "0.7078037537043135\n"
     ]
    }
   ],
   "source": [
    "params = count(num_corpus,word_numbers,tag_numbers)\n",
    "\n",
    "O = params[2]\n",
    "O[tag_numbers[\"USR\"],:] = float('-inf')\n",
    "O[tag_numbers[\"USR\"],word_numbers[\"USER_TOKEN\"]] = 0\n",
    "O[tag_numbers[\"RT\"],:] = float('-inf')\n",
    "O[tag_numbers[\"RT\"],word_numbers[\"RETWEET_TOKEN\"]] = 0\n",
    "O[tag_numbers[\"URL\"],:] = float('-inf')\n",
    "O[tag_numbers[\"URL\"],word_numbers[\"URL_TOKEN\"]] = 0\n",
    "O[tag_numbers[\"HT\"],:] = float('-inf')\n",
    "O[tag_numbers[\"HT\"],word_numbers[\"HASHTAG_TOKEN\"]] = 0\n",
    "\n",
    "predictionTupList = []\n",
    "for tupList in tweet_corpus:\n",
    "    observations = [tup[0] for tup in tupList]\n",
    "    predictionTupList.append(viterbi(params, observations))\n",
    "# print predictionTupList[0]\n",
    "all_test_tags = [tag for tags in getTagSequenceList(tweet_corpus) for tag in tags]\n",
    "    # for predictions, we need to obtain the original tag from the index\n",
    "all_pred_tags = [tag for tags in getTagSequenceList(predictionTupList) for tag in tags]\n",
    "\n",
    "# print acc(all_test_tags, all_pred_tags)\n",
    "\n",
    "def count3(tagged_tup_corpus,vocabulary,tagset): \n",
    "    S = len(tagset)\n",
    "    V = len(vocabulary)\n",
    "\n",
    "    # initalise\n",
    "    eps = 0.1\n",
    "    pi = eps * np.ones(S)\n",
    "    pe = eps * np.ones(S)\n",
    "    A = eps * np.ones((S, S))\n",
    "    O = eps * np.ones((S, V))\n",
    "    B = eps * np.ones((V, S))\n",
    "\n",
    "    # count\n",
    "    for sent in tagged_tup_corpus:\n",
    "        last_tag = None\n",
    "        for word, tag in sent:\n",
    "            O[tag, word] += 1\n",
    "            B[word, tag] +=1\n",
    "            if last_tag == None:\n",
    "                pi[tag] += 1\n",
    "            else:\n",
    "                A[last_tag, tag] += 1\n",
    "            last_tag = tag\n",
    "        pe[last_tag]+=1\n",
    "    # normalise\n",
    "    pi /= np.sum(pi)\n",
    "    for s in range(S):\n",
    "        O[s,:] /= np.sum(O[s,:])\n",
    "        A[s,:] /= np.sum(A[s,:])\n",
    "    for v in range(V):\n",
    "        B[v,:] /= np.sum(B[v,:])\n",
    "#     pi = np.log(pi)\n",
    "#     O = np.log(O)\n",
    "#     A = np.log(A)\n",
    "    return pi,pe,A,O,B\n",
    "    \n",
    "\n",
    "def forward(params,M,S):\n",
    "    pi = params[0]\n",
    "    pe = params[1]\n",
    "    A = params[2]\n",
    "    O = params[3]\n",
    "    B = params[4]\n",
    "    alpha = np.zeros((M, S))\n",
    "    alpha[0, :] = pi * O[:,observations[0]]\n",
    "    # recursive case\n",
    "    for t in range(1, M):\n",
    "        temp = alpha[t-1] * A.T * O.T[observations[t]].reshape(S,1)\n",
    "        alpha[t] = np.matmul(temp,[1]*S)\n",
    "    #TODO add <end> to A\n",
    "    #which one to choose\n",
    "#     total = np.sum(alpha[M-1,:] + A[:,S]) \n",
    "    total = np.sum(alpha[M-1,:] * pe)\n",
    "    return total, alpha\n",
    "    \n",
    "def backforward(params,M,S):\n",
    "    pi = params[0]\n",
    "    pe = params[1]\n",
    "    A = params[2]\n",
    "    O = params[3]\n",
    "    B = params[4]\n",
    "    beta = np.zeros((M, S))\n",
    "    #TODO\n",
    "#     for t in range(S):\n",
    "        \n",
    "#         beta[M, t] = A[:,<end>]\n",
    "#         beta[M-1, t] = A[:,S]\n",
    "        #in order to setvalues not address\n",
    "    beta[M-1] = pe[:]\n",
    "    for i in range(M-2, -1, -1):\n",
    "        temp = beta[i+1] * A * O[:,observations[i+1]]\n",
    "        beta[i] = np.matmul(temp,[1]*S) \n",
    "    total = np.sum(beta[0,:] * pi * O[:,observations[0]]) \n",
    "    return total, beta\n",
    "\n",
    "def expected_count(alpha,totalF,beta,totalB,observations,CT,CWT,CTT,CT2,params,S,V):\n",
    "    M = len(observations)\n",
    "    \n",
    "    pi = params[0]\n",
    "    pe = params[1]\n",
    "    A = params[2]\n",
    "    O = params[3]\n",
    "    B = params[4]\n",
    "    \n",
    "    # count\n",
    "    for i in range(len(observations)):\n",
    "        CT += (alpha[i] * beta[i] /totalF)\n",
    "        CWT[observations[i]] += (alpha[i] * beta[i] /totalF)\n",
    "        if i<len(observations)-1:\n",
    "            temp = alpha[i] * A.T * O[:,observations[i+1]].reshape(S,1)*beta[i+1].reshape(S,1)\n",
    "            CTT +=temp\n",
    "            CT2 += np.matmul([1]*S,temp)\n",
    "            \n",
    "#             for s1 in range(S):\n",
    "#                 temp = alpha[i] * A[:,s1]*O[s1,observations[i+1]]*beta[i+1,s1]/totalF\n",
    "#                 CTT[s1]+=temp\n",
    "#                 CT += temp\n",
    "                \n",
    "#                 for s2 in range(S):\n",
    "#                     temp = alpha[i,s2]*A[s2,s1]*O[s1,observations[i+1]]*beta[i+1,s1]/totalF\n",
    "#                     CTT[s1,s2] += temp\n",
    "#                     CT[s2] += temp\n",
    "        \n",
    "        #TODO when at end\n",
    "#         for s in range(S):\n",
    "#             CT[s] += alpha[i,s] * beta[i,s] / totalF\n",
    "#             CTW[observations[i],s] += (alpha[i,s] * beta[i,s] /totalF)\n",
    "        \n",
    "        \n",
    "#         pe[last_tag]+=1\n",
    "    # normalise\n",
    "    pi /= np.sum(pi)\n",
    "    for s in range(S):\n",
    "        O[s,:] /= np.sum(O[s,:])\n",
    "        A[s,:] /= np.sum(A[s,:])\n",
    "    \n",
    "    return CT,CWT,CTT,CT2\n",
    "\n",
    "\n",
    "# pi = eps * np.ones(S)\n",
    "# pe = eps * np.ones(S)\n",
    "# A = eps * np.ones((S, S))\n",
    "# O = eps * np.ones((S, V))\n",
    "V = len(word_numbers)\n",
    "S = len(tag_numbers)\n",
    "for i in range(5):\n",
    "    eps = 0.1\n",
    "    params = count3(predictionTupList+num_corpus,word_numbers,tag_numbers)\n",
    "    \n",
    "    CT = eps * np.ones(S)\n",
    "    CWT = eps * np.ones((V, S))\n",
    "    CTT = eps * np.ones((S, S))\n",
    "    CT2 = eps * np.ones(S)\n",
    "#     print len(tweet_corpus)\n",
    "#     tcount = 0\n",
    "    for tupList in tweet_corpus:\n",
    "        M = len(tupList)\n",
    "        observations = [tup[0] for tup in tupList]\n",
    "        totalF,alpha = forward(params,M,S)\n",
    "        totalB,beta = backforward(params,M,S)\n",
    "        expected_count(alpha,totalF,beta,totalB,observations,CT,CWT,CTT,CT2,params,S,V)\n",
    "#         tcount+=1\n",
    "#         print tcount\n",
    "    O = (CWT/CT).T\n",
    "    A = (CTT/CT2).T\n",
    "    pi = params[0]\n",
    "    \n",
    "    for s in range(S):\n",
    "        O[s,:] /= np.sum(O[s,:])\n",
    "        A[s,:] /= np.sum(A[s,:])\n",
    "    \n",
    "    pi = np.log(pi)\n",
    "    O = np.log(O)\n",
    "    A = np.log(A)\n",
    "    \n",
    "    \n",
    "    params = [pi, A, O]\n",
    "    \n",
    "    predictionTupList = []\n",
    "    for tupList in tweet_corpus:\n",
    "        observations = [tup[0] for tup in tupList]\n",
    "        predictionTupList.append(viterbi(params, observations))\n",
    "    all_test_tags = [tag for tags in getTagSequenceList(tweet_corpus) for tag in tags]\n",
    "    # for predictions, we need to obtain the original tag from the index\n",
    "    all_pred_tags = [tag for tags in getTagSequenceList(predictionTupList) for tag in tags]\n",
    "    print acc(all_test_tags, all_pred_tags)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
