{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Student Name:Mao Yundong\n",
    "\n",
    "Student ID:882542\n",
    "\n",
    "Python version used:3.6.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "import json\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def myLemmatize(lemmatizer,tWord):\n",
    "    temp = lemmatizer.lemmatize(tWord)\n",
    "    if len(temp) < len(tWord):\n",
    "        return temp\n",
    "    return lemmatizer.lemmatize(tWord,\"v\")\n",
    "# f = open(\"./assignment_group/project_files/testing.json\")\n",
    "# a = f.readline()\n",
    "# dict = json.loads(a)\n",
    "# print (len(dict))\n",
    "# print (dict[0])\n",
    "\n",
    "\n",
    "f2 = open(\"./documents.json\")\n",
    "a = f2.readline()\n",
    "doc_dict = json.loads(a)\n",
    "# print (doc_dict[0])\n",
    "\n",
    "\n",
    "word_numbers = {}\n",
    "num_docs = []\n",
    "num_paras = [] \n",
    "para_doc = {}\n",
    "doc_para = {}\n",
    "wordnum_freq_doc = collections.defaultdict(list)\n",
    "wordnum_freq_para = collections.defaultdict(list)\n",
    "for i,doc in enumerate(doc_dict):\n",
    "    temp_doc = []\n",
    "    temp_doc_set = set()\n",
    "    doc_para[i] = [len(num_paras)]\n",
    "    for j,para in enumerate(doc[\"text\"]):\n",
    "        temp_para_set = set()\n",
    "        temp_para = []\n",
    "        words = re.split(\",|\\.| |\\?|!\",para)\n",
    "        for word in words:\n",
    "            if word.isalnum():\n",
    "                tempword = myLemmatize(lemmatizer,word.lower())\n",
    "                if tempword not in stopWords:\n",
    "                    wi = word_numbers.setdefault(tempword,len(word_numbers))\n",
    "                    temp_para.append(wi)\n",
    "                    temp_doc_set.add(wi)\n",
    "                    temp_para_set.add(wi)\n",
    "        para_doc[len(num_paras)] = (i,j)\n",
    "        num_paras.append(temp_para)\n",
    "        temp_doc.append(temp_para)\n",
    "        for wordnum in temp_para_set:\n",
    "            wordnum_freq_para[wordnum].append((i,j))\n",
    "    doc_para[i].append(len(num_paras))\n",
    "    for wordnum in temp_doc_set:\n",
    "        wordnum_freq_doc[wordnum].append(i)\n",
    "    if i% 50 == 0:\n",
    "        print (i)\n",
    "    num_docs.append(temp_doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First recognized in 1900 by Max Planck, it was originally the proportionality constant between the minimal increment of energy, E, of a hypothetical electrically charged oscillator in a cavity that contained black body radiation, and the frequency, f, of its associated electromagnetic wave', ' In 1905 the value E, the minimal energy increment of a hypothetical oscillator, was theoretically associated by Einstein with a \"quantum\" or minimal element of the energy of the electromagnetic wave itself', ' The light quantum behaved in some respects as an electrically neutral particle, as opposed to an electromagnetic wave', ' It was eventually called the photon', '']\n",
      "First recognized in 1900 by Max Planck, it was originally the proportionality constant between the minimal increment of energy, E, of a hypothetical electrically charged oscillator in a cavity that contained black body radiation, and the frequency, f, of its associated electromagnetic wave\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "# doct_para_sentence = [sent for doc in doc_dict for para in doc[\"text\"] for sent in re.split(\"\\.|\\?|\\!|;\",para)]\n",
    "doct_para_sentence = []\n",
    "doc_para_sentence_number = []\n",
    "for doc in doc_dict:\n",
    "    temppara = []\n",
    "    tempparas_number = []\n",
    "    for para in doc[\"text\"]:\n",
    "        sentences = re.split(\"\\.|\\?|\\!|;\",para)\n",
    "        temppara.append(sentences)\n",
    "        temp_sentences_number = []\n",
    "        for sentence in sentences:\n",
    "            temp_sentence_number = []\n",
    "            words = re.split(\",|\\.| |\\?|!\",sentence)\n",
    "            for word in words:\n",
    "                if word.isalnum():\n",
    "                    tempword = myLemmatize(lemmatizer,word.lower())\n",
    "                    if tempword not in stopWords:\n",
    "                        wi = word_numbers.setdefault(tempword,len(word_numbers))\n",
    "                        temp_para.append(wi)\n",
    "                        temp_doc_set.add(wi)\n",
    "                        temp_para_set.add(wi)\n",
    "                        temp_sentence_number.append(wi)\n",
    "            temp_sentences_number.append(temp_sentence_number)\n",
    "        tempparas_number.append(temp_sentences_number)\n",
    "            \n",
    "    doct_para_sentence.append(temppara)\n",
    "    doc_para_sentence_number.append(tempparas_number)\n",
    "    \n",
    "print(doct_para_sentence[0][0])\n",
    "print(doct_para_sentence[0][0][0])\n",
    "print(doc_para_sentence_number[0][23][5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import coo_matrix\n",
    "import collections\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def getSparseMatrix(list2D,feature_number):\n",
    "    row,column,data = [],[],[]\n",
    "\n",
    "    for j,numList in enumerate(list2D):\n",
    "        tempm = collections.defaultdict(int)\n",
    "        for i in numList:\n",
    "            tempm[i]+=1\n",
    "        for i in tempm.keys():\n",
    "            column.append(i)\n",
    "            data.append(tempm[i])\n",
    "        row.extend([j]*len(tempm))\n",
    "    return coo_matrix((data,(row,column)),shape=(len(list2D),feature_number+1))\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33455\n",
      "29186\n"
     ]
    }
   ],
   "source": [
    "keyword_doc_numset = set([wi for wi in wordnum_freq_doc.keys() if len(wordnum_freq_doc[wi])<=1])\n",
    "keyword_para_numset = set([wi for wi in wordnum_freq_para.keys() if len(wordnum_freq_para[wi])<=1])\n",
    "print (len(keyword_doc_numset))\n",
    "print (len(keyword_para_numset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441\n",
      "59028\n",
      "['first', 'recognize', '1900', 'max', 'planck', 'wa', 'originally', 'proportionality', 'constant', 'minimal', 'increment', 'energy', 'e', 'hypothetical', 'electrically', 'charge', 'oscillator', 'cavity', 'contain', 'black', 'body', 'radiation', 'frequency', 'f', 'associate', 'electromagnetic', 'wave', '1905', 'value', 'theoretically', 'einstein', 'element', 'light', 'quantum', 'behave', 'respect', 'neutral', 'particle', 'oppose', 'eventually', 'call', 'photon', 'classical', 'statistical', 'mechanic', 'require', 'existence', 'h', 'doe', 'define', 'follow', 'upon', 'discovery', 'physical', 'action', 'cannot', 'take', 'arbitrary', 'instead', 'must', 'multiple', 'small', 'quantity', 'physic', 'explain', 'fact', 'many', 'case', 'monochromatic', 'atom', 'also', 'imply', 'certain', 'level', 'allow', 'forbid', 'equivalently', 'smallness', 'reflect', 'everyday', 'object', 'system', 'make', 'large', 'number', 'example', 'green', 'wavelength', '555', 'nanometre', 'approximate', 'human', 'eye', 'ha', 'thz', 'hf', 'j', 'amount', 'term', 'experience']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import coo_matrix\n",
    "import collections\n",
    "import re\n",
    "import numpy as np\n",
    "row,column,data = [],[],[]\n",
    "for j,num_para in enumerate(num_paras):\n",
    "    tempm = collections.defaultdict(int)\n",
    "    for i in num_para:\n",
    "        tempm[i]+=1\n",
    "    for i in tempm.keys():\n",
    "        column.append(i)\n",
    "        data.append(tempm[i])\n",
    "    row.extend([j]*len(tempm))\n",
    "matr_doc = coo_matrix((data,(row,column)),shape=(len(num_paras),len(word_numbers)+1))\n",
    "\n",
    "print (len(doc_dict))\n",
    "print (len(word_numbers))\n",
    "print (list(word_numbers.keys())[:100])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "import spacy\n",
    "import json\n",
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "st = StanfordNERTagger('D:/websearch/stanford/stanford-ner-2015-12-09/stanford-ner-2015-12-09/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "                       ,'D:/websearch/stanford/stanford-ner.jar',\n",
    "                       encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankByAppendDistance(question, sentence):    \n",
    "    question_words_set = set()\n",
    "    for word in question:\n",
    "        tempword = myLemmatize(lemmatizer,word.lower())\n",
    "        if tempword not in stopWords:\n",
    "            question_words_set.add(tempword)\n",
    "    \n",
    "    question_words_index_in_sentence = []\n",
    "    for i,word in enumerate(sentence):\n",
    "        tempword = myLemmatize(lemmatizer,word.lower())\n",
    "        if tempword in question_words_set:\n",
    "            question_words_index_in_sentence.append(i)\n",
    "    \n",
    "    distances = []\n",
    "    for i,word in enumerate(sentence):\n",
    "        tempword = myLemmatize(lemmatizer,word.lower())\n",
    "        distance = 0\n",
    "        if tempword not in question_words_set:\n",
    "            for index in question_words_index_in_sentence:\n",
    "                if index < i:\n",
    "                    distance+=(i-index)\n",
    "                else:\n",
    "                    distance+=(index-i)\n",
    "            distances.append((word,distance,i))\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    return distances\n",
    "\n",
    "def rankByAppendDistance2(ques, sent):    \n",
    "    question_words_set = set()\n",
    "    question = [c.text for c in ques]\n",
    "    sentence = [c.text for c in sent]\n",
    "    \n",
    "    for word in question:\n",
    "        tempword = myLemmatize(lemmatizer,word.lower())\n",
    "        if tempword not in stopWords:\n",
    "            question_words_set.add(tempword)\n",
    "    \n",
    "    question_words_index_in_sentence = []\n",
    "    for i,word in enumerate(sentence):\n",
    "        tempword = myLemmatize(lemmatizer,word.lower())\n",
    "        if tempword in question_words_set:\n",
    "            question_words_index_in_sentence.append(i)\n",
    "    \n",
    "    distances = []\n",
    "    for i,word in enumerate(sentence):\n",
    "        tempword = myLemmatize(lemmatizer,word.lower())\n",
    "        distance = 0\n",
    "        if tempword not in question_words_set:\n",
    "            if sent[i].tag_.startswith(\"NN\") or sent[i].tag_ == \"XX\":\n",
    "                if i < len(sent)-1 and sent[i+1].text == \"of\":\n",
    "                    continue\n",
    "                for index in question_words_index_in_sentence:\n",
    "                    if index < i:\n",
    "                        distance+=(i-index)\n",
    "                    else:\n",
    "                        distance+=(index-i)\n",
    "                distances.append((word,distance,i))\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "    return distances\n",
    "\n",
    "\n",
    "# print (rankByAppendDistance([\"question\"], [\"sentence\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rules_list = [\n",
    "            ('person','PERSON'),\n",
    "            ('location', 'LOCATION'),            \n",
    "            ('from', 'LOCATION'),\n",
    "            ('country', 'LOCATION'),\n",
    "            ('capital', 'LOCATION'),\n",
    "            ('city', 'LOCATION'),\n",
    "            ('many', 'NUMBER'),\n",
    "            ('long','NUMBER'),\n",
    "            ('high', 'NUMBER'),\n",
    "            ('year', 'NUMBER'),\n",
    "            ('decade', 'NUMBER'),\n",
    "            ('time', 'NUMBER'),\n",
    "            ('cost', 'NUMBER'),\n",
    "            ('population', 'NUMBER'),\n",
    "            ('number','NUMBER'),\n",
    "            ('size','NUMBER'),\n",
    "            ('much','NUMBER'),\n",
    "            ('value','NUMBER')\n",
    "        ]\n",
    "\n",
    "\n",
    "rules = [{},{},{}]\n",
    "rules[0][\"who\"] = \"PERSON\";\n",
    "rules[0][\"where\"] = \"LOCATION\"\n",
    "rules[0][\"when\"] = \"NUMBER\"\n",
    "rules[0][\"why\"] = \"OTHER\"\n",
    "\n",
    "\n",
    "\n",
    "for tup in rules_list:\n",
    "    rules[1][tup[0]] = tup[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getParaIndex(question,docid):\n",
    "    words = re.split(\",|\\.| |\\?|!\",question)\n",
    "    num_question = []\n",
    "    for word in words:\n",
    "        if word.isalnum():\n",
    "            tempword = myLemmatize(lemmatizer,word.lower())\n",
    "            if tempword not in stopWords:\n",
    "                num_question.append(word_numbers.get(tempword,len(word_numbers)))\n",
    "    matr = getSparseMatrix([num_question],len(word_numbers))\n",
    "    cos_sims = cosine_similarity(matr,matr_doc)\n",
    "    paraindex = np.argmax(cos_sims[0])\n",
    "    for wi in num_question:\n",
    "        if wi in keyword_para_numset:\n",
    "            if wordnum_freq_para[wi][0][0] == docid:\n",
    "                paraindex = wordnum_freq_para[wi][0][1]\n",
    "                break\n",
    "    else:\n",
    "        start = doc_para[docid][0]\n",
    "        end = doc_para[docid][1]\n",
    "        paraindex = np.argmax(cos_sims[0][start:end])\n",
    "        paraindex = para_doc[paraindex+start][1]\n",
    "    return paraindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAnswer(question,doctext):\n",
    "    tempanswer = \"\"\n",
    "#     doctext = doct_para_sentence[docid][paraid][sentenceid]\n",
    "    tokenized_text = word_tokenize(doctext)\n",
    "    classified_text = st.tag(tokenized_text)\n",
    "\n",
    "    temp = []\n",
    "    for tup in classified_text:\n",
    "        if tup[0].isdigit():\n",
    "            temp.append((tup[0],\"NUMBER\"))\n",
    "        else:\n",
    "            temp.append(tup)\n",
    "    classified_text = temp\n",
    "    new_classified_text = []\n",
    "    temp = classified_text[0][0]\n",
    "    lastNer = classified_text[0][1]\n",
    "    for i in range(1,len(classified_text)):\n",
    "        if classified_text[i][1] in ['PERSON','LOCATION','NUMBER'] and classified_text[i][1] == lastNer:\n",
    "            temp+=(\" \"+classified_text[i][0])\n",
    "        else:\n",
    "            new_classified_text.append((temp,lastNer))\n",
    "            temp = classified_text[i][0]\n",
    "            lastNer = classified_text[i][1]\n",
    "    else:\n",
    "        new_classified_text.append((temp,lastNer))\n",
    "    classified_text = new_classified_text\n",
    "    insent_distances = rankByAppendDistance(word_tokenize(question),[c[0] for c in classified_text])\n",
    "\n",
    "    for temp in insent_distances:\n",
    "        if classified_text[temp[2]][1] == answerType:\n",
    "            tempanswer = classified_text[temp[2]][0].lower()\n",
    "            break  \n",
    "    return tempanswer\n",
    "\n",
    "def getAnswer2(question,doctext):\n",
    "    tempanswer = \"\"\n",
    "    sent = nlp(doctext)\n",
    "    insent_distances = rankByAppendDistance2(ques,sent)\n",
    "    for temp in insent_distances:\n",
    "        if sent[temp[2]].ent_type_ not in answerTypeSet:  \n",
    "            chunk_start = sent[temp[2]].left_edge.i\n",
    "            chunk_end = sent[temp[2]].right_edge.i + 1  \n",
    "            tempanswer = []\n",
    "            for i in range(chunk_start,chunk_end):\n",
    "                tempanswer.append(sent[i].text)\n",
    "                tempanswer.append(\" \")\n",
    "            tempanswer.pop()\n",
    "\n",
    "            for i in range(max(0,chunk_start-2),chunk_start):\n",
    "                if sent[i].text == \"of\":\n",
    "                    temp2 = []\n",
    "                    for j in range(i+1,chunk_start):\n",
    "                        temp2.append(sent[j].text)\n",
    "                        temp2.append(\" \")\n",
    "                    tempanswer =temp2+tempanswer\n",
    "                    break\n",
    "            if tempanswer[0] in [\"the\",\"a\",\"an\"]:\n",
    "                tempanswer =tempanswer[2:]\n",
    "            tempanswer = \"\".join(tempanswer)\n",
    "            tempanswer = tempanswer.lower()\n",
    "            break\n",
    "    return tempanswer\n",
    "\n",
    "def getAnswer3(question,doctext):\n",
    "    tempanswer = \"\"\n",
    "    sent = nlp(doctext)\n",
    "    insent_distances = rankByAppendDistance2(ques,sent)\n",
    "    for temp in insent_distances:\n",
    "        if sent[temp[2]].ent_type_ not in answerTypeSet:  \n",
    "            if sent[temp[2]].text not in set([\"the\",\"a\",\"an\",\"of\"]):\n",
    "                return sent[temp[2]].text\n",
    "    return tempanswer\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3618\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-32a114b8116d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[1;31m#                 tempanswer = getAnswer2(question,\"\".join([sentence+\". \" for para in doct_para_sentence[docid] for sentence in para]))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 73\u001b[1;33m         \u001b[0mtempanswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetAnswer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdoct_para_sentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdocid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparaid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msentenceid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     74\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtempanswer\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mtempanswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetAnswer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m\". \"\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdoct_para_sentence\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdocid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mparaid\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-2b81a59a9f9a>\u001b[0m in \u001b[0;36mgetAnswer\u001b[1;34m(question, doctext)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#     doctext = doct_para_sentence[docid][paraid][sentenceid]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mtokenized_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoctext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mclassified_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtemp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\nltk\\tag\\stanford.py\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;31m# This function should return list of tuple rather than list of list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\nltk\\tag\\stanford.py\u001b[0m in \u001b[0;36mtag_sents\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;31m# Run the tagger and get the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         stanpos_output, _stderr = java(cmd, classpath=(self._stanford_jar,\"D:\\\\websearch\\\\stanford\\\\slf4j-api.jar\"),\n\u001b[1;32m--> 104\u001b[1;33m                                        stdout=PIPE, stderr=PIPE)\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[0mstanpos_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstanpos_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mjava\u001b[1;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[0;32m    129\u001b[0m     \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msubprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstdin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mblocking\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;31m# Check the return code.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcommunicate\u001b[1;34m(self, input, timeout)\u001b[0m\n\u001b[0;32m    841\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    842\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 843\u001b[1;33m                 \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mendtime\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    844\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    845\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_communication_started\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_communicate\u001b[1;34m(self, input, endtime, orig_timeout)\u001b[0m\n\u001b[0;32m   1090\u001b[0m             \u001b[1;31m# calls communicate again.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1092\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_remaining_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mendtime\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1093\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstdout_thread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_alive\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1094\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutExpired\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morig_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\threading.py\u001b[0m in \u001b[0;36mjoin\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             \u001b[1;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m   1070\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlock\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# already determined that the C code is done\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1071\u001b[0m             \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1072\u001b[1;33m         \u001b[1;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1073\u001b[0m             \u001b[0mlock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1074\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "f4 = open(\"./testing.json\")\n",
    "a = f4.readline()\n",
    "f4.close()\n",
    "trains = json.loads(a)\n",
    "print (len(trains))\n",
    "\n",
    "# f2 = open(\"./assignment_group/project_files/training.json\")\n",
    "# a = f2.readline()\n",
    "# f2.close()\n",
    "# trains = json.loads(a)\n",
    "# print (len(trains))\n",
    "\n",
    "\n",
    "import spacy\n",
    "nlp = spacy.load(\"en\")\n",
    "import codecs\n",
    "\n",
    "# file = codecs.open(\"lol\", \"w\", \"utf-8\")\n",
    "# file.write(u'\\ufeff')\n",
    "# file.close()\n",
    "answerTypeSet = set([\"PERSON\",\"LOC\"])\n",
    "\n",
    "\n",
    "f5 = codecs.open(\"./mao001_02.csv\",\"w\",\"utf-8\")\n",
    "f5.writelines(\"id,answer\\n\")\n",
    "\n",
    "count = 0\n",
    "count2 = 0\n",
    "for i in range(0,len(trains)):\n",
    "    question = trains[i][\"question\"]\n",
    "#     paraid = trains[i][\"answer_paragraph\"]\n",
    "    docid = trains[i][\"docid\"]\n",
    "    testid = trains[i][\"id\"]\n",
    "#     answer = trains[i][\"text\"]\n",
    "#     para = doc_dict[docid][\"text\"][paraid]\n",
    "    \n",
    "    paraid = getParaIndex(question,docid)\n",
    "    ques =  nlp(question)\n",
    "    for word in ques:\n",
    "        if word.text.lower() in rules[0]:\n",
    "            answerType = rules[0][word.text.lower()]\n",
    "            break\n",
    "    else:\n",
    "        for word in ques:\n",
    "            if word.text.lower() in rules[1]:\n",
    "                answerType = rules[1][word.text.lower()]\n",
    "                break\n",
    "        else:\n",
    "            answerType = \"OTHER\"\n",
    "\n",
    "    words = re.split(\",|\\.| |\\?|!\",question)\n",
    "    num_question = []\n",
    "    for word in words:\n",
    "        if word.isalnum():\n",
    "            tempword = myLemmatize(lemmatizer,word.lower())\n",
    "            if tempword not in stopWords:\n",
    "                num_question.append(word_numbers.get(tempword,len(word_numbers)))\n",
    "    matr = getSparseMatrix([num_question],len(word_numbers))\n",
    "    matr_para = getSparseMatrix(doc_para_sentence_number[docid][paraid],len(word_numbers))\n",
    "    cos_sims = cosine_similarity(matr,matr_para)\n",
    "    sentenceid = np.argmax(cos_sims[0])\n",
    "    \n",
    "    tempanswer = \"\"\n",
    "    if answerType == \"OTHER\":\n",
    "        tempanswer = getAnswer2(question,doct_para_sentence[docid][paraid][sentenceid])\n",
    "        if len(tempanswer) > 4:\n",
    "            tempanswer = getAnswer3(question,doct_para_sentence[docid][paraid][sentenceid])\n",
    "#         if tempanswer == \"\":\n",
    "#             tempanswer = getAnswer2(question,\"\".join([c+\". \" for c in doct_para_sentence[docid][paraid]]))\n",
    "#             if tempanswer == \"\":\n",
    "#                 tempanswer = getAnswer2(question,\"\".join([sentence+\". \" for para in doct_para_sentence[docid] for sentence in para]))\n",
    "    else:\n",
    "        tempanswer = getAnswer(question,doct_para_sentence[docid][paraid][sentenceid])\n",
    "        if tempanswer == \"\":\n",
    "            tempanswer = getAnswer(question,\"\".join([c+\". \" for c in doct_para_sentence[docid][paraid]]))\n",
    "            if tempanswer == \"\":\n",
    "                tempanswer = getAnswer(question,\"\".join([sentence+\". \" for para in doct_para_sentence[docid] for sentence in para]))\n",
    "        if tempanswer == \"\":\n",
    "            print(i,\"--------\")\n",
    "        \n",
    "    f5.writelines(str(testid)+\",\"+tempanswer+\"\\n\");\n",
    "    print(count)\n",
    "#     if count> 5:\n",
    "#         break\n",
    "    count+=1\n",
    "f5.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdfasdf\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "f = codecs.open(\"./mao001_02.csv\",\"r\",\"utf-8\")\n",
    "strings = []\n",
    "while True:\n",
    "    temp = f.readline()\n",
    "    if not temp:\n",
    "        break\n",
    "    strings.append(temp)\n",
    "for i in range(1,len(strings)):\n",
    "    start = strings[i].index(\",\")\n",
    "    temp = strings[i][start+1:]\n",
    "#     if temp == \"unk\\n\":\n",
    "#         print(\"asdf\")\n",
    "#         strings[i] =  strings[i][:start+1]+\"\\n\"\n",
    "#         continue\n",
    "    if temp == \"\\n\":\n",
    "        strings[i] =  strings[i][:start+1]+\"\\n\"\n",
    "        continue\n",
    "    temp = temp.replace(\"\\\"\",\"\").replace(\":\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\",\",\"\").strip()\n",
    "#     if temp == \"\":\n",
    "#         strings[i] =  strings[i][:start+1]+\"\\n\"\n",
    "#         continue\n",
    "    temp2 = [temp[0]]\n",
    "    for j in range(1,len(temp)):\n",
    "        if temp[j] == temp[j-1] and temp[j] == \" \":\n",
    "            continue\n",
    "        else:\n",
    "            temp2.append(temp[j])\n",
    "    temp = \"\".join(temp2)\n",
    "#     print(temp+\"-----------------\")\n",
    "    strings[i] =  strings[i][:start+1]+temp+\"\\n\"\n",
    "f.close()\n",
    "\n",
    "f = codecs.open(\"./prediction001_01.csv\",\"w\",\"utf-8\")\n",
    "for temp in strings:\n",
    "    f.writelines(temp)\n",
    "f.close()\n",
    "\n",
    "print(\"asdfasdf\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdfasdf\n"
     ]
    }
   ],
   "source": [
    "# import codecs\n",
    "# f = codecs.open(\"./assignment_group/project_files/mao001_02.csv\",\"r\",\"utf-8\")\n",
    "# strings = []\n",
    "# while True:\n",
    "#     temp = f.readline()\n",
    "#     if not temp:\n",
    "#         break\n",
    "#     strings.append(temp)\n",
    "# f.close()\n",
    "# f = codecs.open(\"./assignment_group/project_files/mao002_03.csv\",\"r\",\"utf-8\")\n",
    "# f.readline()\n",
    "# while True:\n",
    "#     temp = f.readline()\n",
    "#     if not temp:\n",
    "#         break\n",
    "#     start = temp.index(\",\")\n",
    "#     testid = int(temp[:start])\n",
    "#     strings[testid+1] = temp\n",
    "# f.close()\n",
    "\n",
    "# for i in range(1,len(strings)):\n",
    "#     start = strings[i].index(\",\")\n",
    "#     temp = strings[i][start+1:]\n",
    "#     if temp == \"\\n\":\n",
    "#         strings[i] =  strings[i][:start+1]+\"\\n\"\n",
    "#         continue\n",
    "#     temp = temp.replace(\"\\\"\",\"\").replace(\":\",\"\").replace(\"(\",\"\").replace(\")\",\"\").replace(\",\",\"\").strip()\n",
    "#     temp2 = [temp[0]]\n",
    "#     for j in range(1,len(temp)):\n",
    "#         if temp[j] == temp[j-1] and temp[j] == \" \":\n",
    "#             continue\n",
    "#         else:\n",
    "#             temp2.append(temp[j])\n",
    "#     temp = \"\".join(temp2)\n",
    "#     strings[i] =  strings[i][:start+1]+temp+\"\\n\"\n",
    "\n",
    "# f = codecs.open(\"./assignment_group/project_files/prediction002_05.csv\",\"w\",\"utf-8\")\n",
    "# for temp in strings:\n",
    "#     f.writelines(temp)\n",
    "# f.close()\n",
    "\n",
    "# print(\"asdfasdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
