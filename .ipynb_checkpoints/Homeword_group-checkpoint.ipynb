{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Student Name:Mao Yundong\n",
    "\n",
    "Student ID:882542\n",
    "\n",
    "Python version used:3.6.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdffds\n",
      "0\n",
      "50\n",
      "100\n",
      "150\n",
      "200\n",
      "250\n",
      "300\n",
      "350\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import collections\n",
    "from nltk.corpus import stopwords\n",
    "stopWords = set(stopwords.words('english'))\n",
    "import json\n",
    "import re\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "print (\"asdffds\")\n",
    "def myLemmatize(lemmatizer,tWord):\n",
    "    temp = lemmatizer.lemmatize(tWord)\n",
    "    if len(temp) < len(tWord):\n",
    "        return temp\n",
    "    return lemmatizer.lemmatize(tWord,\"v\")\n",
    "# f = open(\"./assignment_group/project_files/testing.json\")\n",
    "# a = f.readline()\n",
    "# dict = json.loads(a)\n",
    "# print (len(dict))\n",
    "# print (dict[0])\n",
    "\n",
    "\n",
    "f2 = open(\"./assignment_group/project_files/documents.json\")\n",
    "a = f2.readline()\n",
    "doc_dict = json.loads(a)\n",
    "# print (doc_dict[0])\n",
    "\n",
    "\n",
    "word_numbers = {}\n",
    "num_docs = []\n",
    "num_paras = [] \n",
    "para_doc = {}\n",
    "doc_para = {}\n",
    "wordnum_freq_doc = collections.defaultdict(list)\n",
    "wordnum_freq_para = collections.defaultdict(list)\n",
    "for i,doc in enumerate(doc_dict):\n",
    "    temp_doc = []\n",
    "    temp_doc_set = set()\n",
    "    doc_para[i] = [len(num_paras)]\n",
    "    for j,para in enumerate(doc[\"text\"]):\n",
    "        temp_para_set = set()\n",
    "        temp_para = []\n",
    "        words = re.split(\",|\\.| |\\?|!\",para)\n",
    "        for word in words:\n",
    "            if word.isalpha():\n",
    "                tempword = myLemmatize(lemmatizer,word.lower())\n",
    "                if tempword not in stopWords:\n",
    "                    wi = word_numbers.setdefault(tempword,len(word_numbers))\n",
    "                    temp_para.append(wi)\n",
    "                    temp_doc_set.add(wi)\n",
    "                    temp_para_set.add(wi)\n",
    "        para_doc[len(num_paras)] = (i,j)\n",
    "        num_paras.append(temp_para)\n",
    "        temp_doc.append(temp_para)\n",
    "        for wordnum in temp_para_set:\n",
    "            wordnum_freq_para[wordnum].append((i,j))\n",
    "    doc_para[i].append(len(num_paras))\n",
    "    for wordnum in temp_doc_set:\n",
    "        wordnum_freq_doc[wordnum].append(i)\n",
    "    if i% 50 == 0:\n",
    "        print (i)\n",
    "    num_docs.append(temp_doc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\毛东东\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32359\n",
      "28173\n"
     ]
    }
   ],
   "source": [
    "keyword_doc_numset = set([wi for wi in wordnum_freq_doc.keys() if len(wordnum_freq_doc[wi])<=1])\n",
    "keyword_para_numset = set([wi for wi in wordnum_freq_para.keys() if len(wordnum_freq_para[wi])<=1])\n",
    "print (len(keyword_doc_numset))\n",
    "print (len(keyword_para_numset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "441\n",
      "55903\n",
      "['first', 'recognize', 'max', 'planck', 'wa', 'originally', 'proportionality', 'constant', 'minimal', 'increment', 'energy', 'e', 'hypothetical', 'electrically', 'charge', 'oscillator', 'cavity', 'contain', 'black', 'body', 'radiation', 'frequency', 'f', 'associate', 'electromagnetic', 'wave', 'value', 'theoretically', 'einstein', 'element', 'light', 'quantum', 'behave', 'respect', 'neutral', 'particle', 'oppose', 'eventually', 'call', 'photon', 'classical', 'statistical', 'mechanic', 'require', 'existence', 'h', 'doe', 'define', 'follow', 'upon', 'discovery', 'physical', 'action', 'cannot', 'take', 'arbitrary', 'instead', 'must', 'multiple', 'small', 'quantity', 'physic', 'explain', 'fact', 'many', 'case', 'monochromatic', 'atom', 'also', 'imply', 'certain', 'level', 'allow', 'forbid', 'equivalently', 'smallness', 'reflect', 'everyday', 'object', 'system', 'make', 'large', 'number', 'example', 'green', 'wavelength', 'nanometre', 'approximate', 'human', 'eye', 'ha', 'thz', 'hf', 'j', 'amount', 'term', 'experience', 'concern', 'individual', 'molecule']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from scipy.sparse import coo_matrix\n",
    "import collections\n",
    "import numpy as np\n",
    "row,column,data = [],[],[]\n",
    "for j,num_para in enumerate(num_paras):\n",
    "    tempm = collections.defaultdict(int)\n",
    "    for i in num_para:\n",
    "        tempm[i]+=1\n",
    "    for i in tempm.keys():\n",
    "        column.append(i)\n",
    "        data.append(tempm[i])\n",
    "    row.extend([j]*len(tempm))\n",
    "matr_doc = coo_matrix((data,(row,column)),shape=(len(num_paras),len(word_numbers)+1))\n",
    "\n",
    "print (len(doc_dict))\n",
    "print (len(word_numbers))\n",
    "print (list(word_numbers.keys())[:100])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3097\n",
      "0\n",
      "30\n",
      "60\n",
      "90\n",
      "0\n",
      "65\n",
      "3097\n"
     ]
    }
   ],
   "source": [
    "f3 = open(\"./assignment_group/project_files/devel.json\")\n",
    "a = f3.readline()\n",
    "f3.close()\n",
    "devel = json.loads(a)\n",
    "count_doc = 0\n",
    "count_para = 0\n",
    "print (len(devel))\n",
    "for j,qa in enumerate(devel[:100]):\n",
    "    question = qa[\"question\"]\n",
    "    paraid = qa[\"answer_paragraph\"]\n",
    "    docid = qa[\"docid\"]\n",
    "    answer = qa[\"text\"]\n",
    "    words = re.split(\",|\\.| |\\?|!\",question)\n",
    "    num_question = []\n",
    "    for word in words:\n",
    "        if word.isalpha():\n",
    "            tempword = myLemmatize(lemmatizer,word.lower())\n",
    "            if tempword not in stopWords:\n",
    "                num_question.append(word_numbers.get(tempword,len(word_numbers)))\n",
    "    row,column,data = [],[],[]\n",
    "    tempm = collections.defaultdict(int)\n",
    "    for i in num_question:\n",
    "        tempm[i]+=1\n",
    "    for i in tempm.keys():\n",
    "        column.append(i)\n",
    "        data.append(tempm[i])\n",
    "    row.extend([0]*len(tempm))\n",
    "    matr = coo_matrix((data,(row,column)),shape=(1,len(word_numbers)+1))\n",
    "    cos_sims = cosine_similarity(matr,matr_doc)\n",
    "    paraindex = np.argmax(cos_sims[0])\n",
    "#     for wi in num_question:\n",
    "#         if wi in keyword_para_numset:\n",
    "#             if docid ==  wordnum_freq_para[wi][0][0]:\n",
    "#                 count_doc+=1\n",
    "#                 if paraid == wordnum_freq_para[wi][0][1]:\n",
    "#                     count_para+=1\n",
    "#             break\n",
    "#     else:\n",
    "    start = doc_para[docid][0]\n",
    "    end = doc_para[docid][1]\n",
    "    paraindex = np.argmax(cos_sims[0][start:end])\n",
    "    paraindex2 = np.argmax(np.concatenate((cos_sims[0][start:end][:paraindex],cos_sims[0][start:end][paraindex+1:])))\n",
    "#         paraindex2 = np.argmax(cos_sims[0][start:end][:paraindex]+cos_sims[0][start:end][paraindex+1:])\n",
    "    if paraid == para_doc[paraindex+start][1]:\n",
    "        count_para+=1\n",
    "    elif paraid == para_doc[paraindex2+start][1]:\n",
    "        count_para+=1\n",
    "    \n",
    "#     for wi in num_question:\n",
    "#         if wi in keyword_para_numset:\n",
    "#             if docid ==  wordnum_freq_para[wi][0][0]:\n",
    "#                 count_doc+=1\n",
    "#                 if paraid == wordnum_freq_para[wi][0][1]:\n",
    "#                     count_para+=1\n",
    "#             break\n",
    "#     else:\n",
    "#         for wi in num_question:\n",
    "#             if wi in keyword_doc_numset:\n",
    "#     #             print wordnum_freq_doc[wi][0]\n",
    "#                 start = doc_para[wordnum_freq_doc[wi][0]][0]\n",
    "#                 end = doc_para[wordnum_freq_doc[wi][0]][1]\n",
    "#                 paraindex = np.argmax(cos_sims[0][start:end])\n",
    "#                 if docid ==  para_doc[paraindex+start][0]:\n",
    "#                     count_doc+=1\n",
    "#                     if paraid == para_doc[paraindex+start][1]:\n",
    "#                         count_para+=1\n",
    "#                 break\n",
    "#         else:\n",
    "#             if docid ==  para_doc[paraindex][0]:\n",
    "#                 count_doc+=1\n",
    "#                 if paraid == para_doc[paraindex][1]:\n",
    "#                     count_para+=1\n",
    "    if j%30==0:\n",
    "        print (j)\n",
    "print (count_doc)\n",
    "print (count_para)\n",
    "print (len(devel))\n",
    "#     print cos_sims[0][paraindex:paraindex+10]\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\毛东东\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\毛东东\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\毛东东\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\毛东东\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "# print nltk.ne_chunk(nltk.corpus.treebank.tagged_sents()[11])\n",
    "nltk.download('punkt')\n",
    "# from nltk.tag import pos_tag\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# print pos_tag(word_tokenize(\"John's big idea isn't all that bad.\"))\n",
    "# import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('words')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "On what date did the companies that became the Computing-Tabulating-Recording Company get consolidated?\n",
      "[('What', 'WP'), ('percentage', 'NN'), ('of', 'IN'), ('its', 'PRP$'), ('desktop', 'JJ'), ('PCs', 'NN'), ('does', 'VBZ'), ('IBM', 'NNP'), ('plan', 'NN'), ('to', 'TO'), ('install', 'VB'), ('Open', 'NNP'), ('Client', 'NNP'), ('on', 'IN'), ('to', 'TO'), ('?', '.')]\n",
      "(S\n",
      "  What/WP\n",
      "  percentage/NN\n",
      "  of/IN\n",
      "  its/PRP$\n",
      "  desktop/JJ\n",
      "  (ORGANIZATION PCs/NN)\n",
      "  does/VBZ\n",
      "  (ORGANIZATION IBM/NNP)\n",
      "  plan/NN\n",
      "  to/TO\n",
      "  install/VB\n",
      "  (PERSON Open/NNP Client/NNP)\n",
      "  on/IN\n",
      "  to/TO\n",
      "  ?/.)\n",
      "(S\n",
      "  What/WP\n",
      "  (NP percentage/NN)\n",
      "  of/IN\n",
      "  (NP its/PRP$ desktop/JJ PCs/NN)\n",
      "  does/VBZ\n",
      "  (NP IBM/NNP plan/NN)\n",
      "  to/TO\n",
      "  install/VB\n",
      "  (NP Open/NNP Client/NNP)\n",
      "  on/IN\n",
      "  to/TO\n",
      "  ?/.)\n",
      "Word: ('What', 'WP')\n",
      "Label: NP\n",
      "Leaves: [('percentage', 'NN')]\n",
      "Word: ('percentage', 'NN')\n",
      "Word: ('of', 'IN')\n",
      "Label: NP\n",
      "Leaves: [('its', 'PRP$'), ('desktop', 'JJ'), ('PCs', 'NN')]\n",
      "Word: ('its', 'PRP$')\n",
      "Word: ('desktop', 'JJ')\n",
      "Word: ('PCs', 'NN')\n",
      "Word: ('does', 'VBZ')\n",
      "Label: NP\n",
      "Leaves: [('IBM', 'NNP'), ('plan', 'NN')]\n",
      "Word: ('IBM', 'NNP')\n",
      "Word: ('plan', 'NN')\n",
      "Word: ('to', 'TO')\n",
      "Word: ('install', 'VB')\n",
      "Label: NP\n",
      "Leaves: [('Open', 'NNP'), ('Client', 'NNP')]\n",
      "Word: ('Open', 'NNP')\n",
      "Word: ('Client', 'NNP')\n",
      "Word: ('on', 'IN')\n",
      "Word: ('to', 'TO')\n",
      "Word: ('?', '.')\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "f3 = open(\"./assignment_group/project_files/devel.json\")\n",
    "a = f3.readline()\n",
    "f3.close()\n",
    "devel = json.loads(a)\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "b = pos_tag(word_tokenize(devel[1][\"question\"]))\n",
    "print (devel[0][\"question\"])\n",
    "print (b)\n",
    "print (nltk.ne_chunk(b))\n",
    "root  = nltk.ne_chunk(b)\n",
    "# if type(root) is nltk.Tree:\n",
    "#     print \"------\"\n",
    "#     print root.label()\n",
    "#     print \"------\"\n",
    "#     print root.leaves()\n",
    "grammar = r\"NP: {<[CDJNP].*>+}\"\n",
    "#TODO\n",
    "cp = nltk.RegexpParser(grammar)\n",
    "result = cp.parse(b)\n",
    "print (result)\n",
    "\n",
    "def getNodes(parent):\n",
    "    for node in parent:\n",
    "#         print type(node)\n",
    "        if type(node) is nltk.Tree:\n",
    "            if node.label() == node:\n",
    "                print (\"======== Sentence =========\")\n",
    "                print (\"Sentence:\", \" \".join(node.leaves()))\n",
    "            else:\n",
    "                print (\"Label:\", node.label())\n",
    "                print (\"Leaves:\", node.leaves())\n",
    "\n",
    "            getNodes(node)\n",
    "        else:\n",
    "            print (\"Word:\", node)\n",
    "\n",
    "getNodes(result)\n",
    "# print devel[0][\"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "28\n",
      "[-2.4183218  -0.7700025   0.39351952  2.6235843   1.995839   -1.3643907\n",
      " -0.5738242   1.155701   -1.0765399   1.7825425  -1.1697105   0.06971284\n",
      " -2.0900254  -0.07137149  0.70699847 -0.7539422   3.4551535   0.6422943\n",
      "  0.43637753 -0.47551757  0.40616605 -1.6444353  -1.9553356   1.3957288\n",
      " -0.6918805  -0.16163789 -0.95779884 -0.39391643]\n",
      "[ 0.25520676 -0.02365677  0.2524145   0.20869294 -0.04920257  0.07489824\n",
      "  0.16527224  0.03057414  0.14632986  0.02486908 -0.06877342 -0.04316078\n",
      " -0.04470331  0.10107683 -0.21038398 -0.2549741   0.21840368 -0.19712389\n",
      "  0.04263122 -0.26919296  0.0542332  -0.06757405 -0.07743678 -0.1689456\n",
      " -0.17024069  0.05495913  0.08258761  0.02054949]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "# import json\n",
    "# f2 = open(\"./assignment_group/project_files/documents.json\")\n",
    "# a = f2.readline()\n",
    "# f2.close()\n",
    "# doc_dict = json.loads(a)\n",
    "# temp_corpus = [word_tokenize(para) for doc in doc_dict for para in doc[\"text\"]]\n",
    "\n",
    "# f3 = open(\"./assignment_group/project_files/devel.json\")\n",
    "# a = f3.readline()\n",
    "# f3.close()\n",
    "# devels = json.loads(a)\n",
    "# temp_corpus += [word_tokenize(question) for devel in devels for question in devel[\"question\"]]\n",
    "\n",
    "# f1 = open(\"./assignment_group/project_files/testing.json\")\n",
    "# a = f1.readline()\n",
    "# f1.close()\n",
    "# tests = json.loads(a)\n",
    "# temp_corpus += [word_tokenize(question) for test in tests for question in test[\"question\"]]\n",
    "\n",
    "# model = gensim.models.Word2Vec(temp_corpus,size = 28)\n",
    "# model.save('document.embedding')\n",
    "new_model = []\n",
    "new_model = gensim.models.Word2Vec.load('d:/websearch/websearch/document.embedding')\n",
    "print (\"--------------------\")\n",
    "print (len(new_model.wv[\"company\"]))\n",
    "print (new_model.wv[\"company\"])\n",
    "print (new_model.wv[\"liked\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'The', u'Fulton', u'County', u'Grand', u'Jury', u'said', u'Friday', u'an', u'investigation', u'of', u\"Atlanta's\", u'recent', u'primary', u'election', u'produced', u'``', u'no', u'evidence', u\"''\", u'that', u'any', u'irregularities', u'took', u'place', u'.']\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CategorizedTaggedCorpusReader' object has no attribute 'vocab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-a1ba4f8cc67b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0mbrown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# model = gensim.models.Word2Vec(brown.sents())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# model.save('brown.embedding')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'CategorizedTaggedCorpusReader' object has no attribute 'vocab'"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import brown\n",
    "print brown.sents()[0]\n",
    "print brown.vocab\n",
    "# model = gensim.models.Word2Vec(brown.sents())\n",
    "# model.save('brown.embedding')\n",
    "# new_model = gensim.models.Word2Vec.load('brown.embedding')\n",
    "# print len(new_model[\"university\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print (28 * 28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43379\n",
      "-------------- ('cylinder', 'NN')\n",
      "-------------- ('1913', 'CD')\n",
      "-------------- ('Kirchhoff', 'NNP')\n",
      "-------------- PERSON kirchhoff\n",
      "-------------- ('Schrödinger', 'NNP')\n",
      "-------------- ('h', 'NN')\n",
      "-------------- ('h', 'NN')\n",
      "-------------- ('f', 'NN')\n",
      "-------------- ('1839', 'CD')\n",
      "-------------- ('1911', 'CD')\n",
      "-------------- ('1911', 'CD')\n",
      "-------------- PERSON heinrich hertz\n",
      "-------------- PERSON alexandre edmond becquerel\n",
      "-------------- PERSON paul ehrenfest\n",
      "-------------- ('Einstein', 'NNP')\n",
      "-------------- PERSON einstein\n",
      "-------------- PERSON alexandre edmond becquerel\n",
      "-------------- ('1911', 'CD')\n",
      "-------------- ('1911', 'CD')\n",
      "-------------- ('Einstein', 'NNP')\n",
      "-------------- PERSON einstein\n",
      "-------------- ('thermal', 'JJ')\n",
      "-------------- ('quantum', 'NN')\n",
      "-------------- ('quantum', 'NN')\n",
      "-------------- ('forbidden', 'JJ')\n",
      "-------------- ('particles', 'NNS')\n",
      "-------------- ('green', 'JJ')\n",
      "-------------- ('green', 'JJ')\n",
      "-------------- ('Einstein', 'NNP')\n",
      "-------------- PERSON einstein\n",
      "-------------- ('colour', 'NN')\n",
      "-------------- ('intensity', 'NN')\n",
      "-------------- ('intensity', 'NN')\n",
      "-------------- ('sound', 'NN')\n",
      "-------------- ('thermal', 'JJ')\n",
      "-------------- ('relativity', 'NN')\n",
      "-------------- ('relativity', 'NN')\n",
      "-------------- ('1918', 'CD')\n",
      "-------------- ('black-body', 'NN')\n",
      "-------------- ('h', 'NN')\n",
      "-------------- ('h', 'NN')\n",
      "-------------- ('h', 'NN')\n",
      "-------------- PERSON wilhelm wien\n",
      "-------------- ('Bohr', 'NNP')\n",
      "-------------- PERSON bohr\n",
      "-------------- ('Bohr', 'NNP')\n",
      "-------------- PERSON bohr\n",
      "-------------- ('Bohr', 'NNP')\n",
      "-------------- ORGANIZATION bohr\n",
      "-------------- ('Bohr', 'NNP')\n",
      "-------------- ORGANIZATION bohr\n",
      "-------------- ('Heisenberg', 'NNP')\n",
      "-------------- PERSON heisenberg\n",
      "-------------- ('platinum–iridium', 'NN')\n",
      "-------------- ('2007', 'CD')\n",
      "-------------- ('intensity', 'NN')\n",
      "-------------- ('intensity', 'NN')\n",
      "-------------- ('1921', 'CD')\n",
      "-------------- ('Heisenberg', 'NNP')\n",
      "-------------- PERSON heisenberg\n",
      "-------------- ('1905', 'CD')\n",
      "-------------- PERSON max planck\n",
      "-------------- ('forty', 'NN')\n",
      "-------------- ('1911', 'CD')\n",
      "-------------- ('1911', 'CD')\n",
      "-------------- ('light', 'JJ')\n",
      "-------------- ('light', 'NN')\n",
      "-------------- ('light', 'JJ')\n",
      "-------------- ('light', 'JJ')\n",
      "-------------- ('hotter', 'JJR')\n",
      "-------------- ('accurate', 'JJ')\n",
      "-------------- ('silicon', 'NN')\n",
      "-------------- ORGANIZATION international energy agency\n",
      "-------------- LOCATION stern review\n",
      "-------------- ('2006', 'CD')\n",
      "-------------- ('2006', 'CD')\n",
      "-------------- ('Solyndra', 'NNP')\n",
      "-------------- GPE solyndra\n",
      "-------------- ('Germany', 'NNP')\n",
      "-------------- GPE germany\n",
      "-------------- ('Germany', 'NNP')\n",
      "-------------- GPE germany\n",
      "-------------- ('three', 'CD')\n",
      "-------------- ('Three', 'NNP')\n"
     ]
    }
   ],
   "source": [
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def getNodes(parent,target):\n",
    "    result = []\n",
    "    lalalala = 0\n",
    "    for node in parent:\n",
    "        if type(node) is nltk.Tree:\n",
    "            if node.label() == node:\n",
    "                print (\"======== Sentence =========\")\n",
    "#                 print (\"Sentence:\", \" \".join(node.leaves()))\n",
    "            else:\n",
    "                lalalala+=1\n",
    "#                 print (\"Label:\", node.label())\n",
    "#                 print (\"Leaves:\", node.leaves())\n",
    "            text = getNodes(node,target)\n",
    "            if text.lower() == target:\n",
    "                print (\"--------------\",node.label(),target)\n",
    "            result.append(text)\n",
    "            result.append(\" \")\n",
    "        else:\n",
    "#             print (\"Word:\", node)\n",
    "            if node[0].lower() == target:\n",
    "                print (\"--------------\",node)\n",
    "            result.append(node[0])\n",
    "            result.append(\" \")\n",
    "    result.pop()\n",
    "    return \"\".join(result)\n",
    "\n",
    "# f2 = open(\"./assignment_group/project_files/documents.json\")\n",
    "# a = f2.readline()\n",
    "# f2.close()\n",
    "# doc_dict = json.loads(a)\n",
    "\n",
    "# f4 = open(\"./assignment_group/project_files/training.json\")\n",
    "# a = f4.readline()\n",
    "# f4.close()\n",
    "# trains = json.loads(a)\n",
    "print (len(trains))\n",
    "# print (trains[0])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "count = 0\n",
    "for train in trains:\n",
    "    question = train[\"question\"]\n",
    "    paraid = train[\"answer_paragraph\"]\n",
    "    docid = train[\"docid\"]\n",
    "    answer = train[\"text\"]\n",
    "    para = doc_dict[docid][\"text\"][paraid]\n",
    "#     print (question,answer)\n",
    "#     print(para)\n",
    "    temp_tag_para = pos_tag(word_tokenize(para))\n",
    "    root = nltk.ne_chunk(temp_tag_para)\n",
    "    getNodes(root,answer)\n",
    "    \n",
    "    count+=1\n",
    "    if count >100:\n",
    "        break\n",
    "\n",
    "\n",
    "# f3 = open(\"./assignment_group/project_files/devel.json\")\n",
    "# a = f3.readline()\n",
    "# f3.close()\n",
    "# devels = json.loads(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asdfasdf\n",
      "CRFClassifier invoked on Wed May 16 01:22:55 AEST 2018 with arguments:\n",
      "   -loadClassifier D:/websearch/stanford/stanford-ner-2015-12-09/stanford-ner-2015-12-09/classifiers/english.all.3class.distsim.crf.ser.gz -textFile C:\\Users\\毛东东\\AppData\\Local\\Temp\\tmpi5m_00qa -outputFormat slashTags -tokenizerFactory edu.stanford.nlp.process.WhitespaceTokenizer -tokenizerOptions \"tokenizeNLs=false\" -encoding utf-8\n",
      "tokenizerFactory=edu.stanford.nlp.process.WhitespaceTokenizer\n",
      "tokenizerOptions=\"tokenizeNLs=false\"\n",
      "loadClassifier=D:/websearch/stanford/stanford-ner-2015-12-09/stanford-ner-2015-12-09/classifiers/english.all.3class.distsim.crf.ser.gz\n",
      "encoding=utf-8\n",
      "textFile=C:\\Users\\毛东东\\AppData\\Local\\Temp\\tmpi5m_00qa\n",
      "outputFormat=slashTags\n",
      "Exception in thread \"main\" java.lang.NoClassDefFoundError: org/slf4j/LoggerFactory\n",
      "\tat edu.stanford.nlp.io.IOUtils.<clinit>(IOUtils.java:42)\n",
      "\tat edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifier(AbstractSequenceClassifier.java:1484)\n",
      "\tat edu.stanford.nlp.ie.AbstractSequenceClassifier.loadClassifierNoExceptions(AbstractSequenceClassifier.java:1497)\n",
      "\tat edu.stanford.nlp.ie.crf.CRFClassifier.main(CRFClassifier.java:3015)\n",
      "Caused by: java.lang.ClassNotFoundException: org.slf4j.LoggerFactory\n",
      "\tat java.net.URLClassLoader.findClass(URLClassLoader.java:381)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:424)\n",
      "\tat sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:349)\n",
      "\tat java.lang.ClassLoader.loadClass(ClassLoader.java:357)\n",
      "\t... 4 more\n",
      "Picked up JAVA_TOOL_OPTIONS: -Dfile.encoding=UTF-8\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Java command failed : ['C:\\\\Program Files\\\\Java\\\\jdk1.8.0_171\\\\bin\\\\java.exe', '-mx1000m', '-cp', 'D:/websearch/stanford/stanford-ner.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', 'D:/websearch/stanford/stanford-ner-2015-12-09/stanford-ner-2015-12-09/classifiers/english.all.3class.distsim.crf.ser.gz', '-textFile', 'C:\\\\Users\\\\毛东东\\\\AppData\\\\Local\\\\Temp\\\\tmpi5m_00qa', '-outputFormat', 'slashTags', '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions', '\"tokenizeNLs=false\"', '-encoding', 'utf-8']",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-ac24d38db025>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mtokenized_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mclassified_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtokenized_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassified_text\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\nltk\\tag\\stanford.py\u001b[0m in \u001b[0;36mtag\u001b[1;34m(self, tokens)\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokens\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;31m# This function should return list of tuple rather than list of list\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtag_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtokens\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtag_sents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\nltk\\tag\\stanford.py\u001b[0m in \u001b[0;36mtag_sents\u001b[1;34m(self, sentences)\u001b[0m\n\u001b[0;32m    102\u001b[0m         \u001b[1;31m# Run the tagger and get the output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m         stanpos_output, _stderr = java(cmd, classpath=self._stanford_jar,\n\u001b[1;32m--> 104\u001b[1;33m                                        stdout=PIPE, stderr=PIPE)\n\u001b[0m\u001b[0;32m    105\u001b[0m         \u001b[0mstanpos_output\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstanpos_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\program files\\python36\\lib\\site-packages\\nltk\\__init__.py\u001b[0m in \u001b[0;36mjava\u001b[1;34m(cmd, classpath, stdin, stdout, stderr, blocking)\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreturncode\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    135\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decode_stdoutdata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 136\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mOSError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Java command failed : '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    137\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Java command failed : ['C:\\\\Program Files\\\\Java\\\\jdk1.8.0_171\\\\bin\\\\java.exe', '-mx1000m', '-cp', 'D:/websearch/stanford/stanford-ner.jar', 'edu.stanford.nlp.ie.crf.CRFClassifier', '-loadClassifier', 'D:/websearch/stanford/stanford-ner-2015-12-09/stanford-ner-2015-12-09/classifiers/english.all.3class.distsim.crf.ser.gz', '-textFile', 'C:\\\\Users\\\\毛东东\\\\AppData\\\\Local\\\\Temp\\\\tmpi5m_00qa', '-outputFormat', 'slashTags', '-tokenizerFactory', 'edu.stanford.nlp.process.WhitespaceTokenizer', '-tokenizerOptions', '\"tokenizeNLs=false\"', '-encoding', 'utf-8']"
     ]
    }
   ],
   "source": [
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "st = StanfordNERTagger('D:/websearch/stanford/stanford-ner-2015-12-09/stanford-ner-2015-12-09/classifiers/english.all.3class.distsim.crf.ser.gz'\n",
    "                       ,'D:/websearch/stanford/stanford-ner.jar',\n",
    "                       encoding='utf-8')\n",
    "print (\"asdfasdf\")\n",
    "# text = 'While in France, Christine Lagarde discussed short-term stimulus efforts in a recent interview with the Wall Street Journal.'\n",
    "text = 'While in France'\n",
    "\n",
    "tokenized_text = word_tokenize(text)\n",
    "classified_text = st.tag(tokenized_text)\n",
    "\n",
    "print(classified_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asldkjfkas\n"
     ]
    }
   ],
   "source": [
    "print(\"asldkjfkas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
