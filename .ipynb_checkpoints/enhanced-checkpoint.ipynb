{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import operator\n",
    "from nltk import word_tokenize\n",
    "from math import log\n",
    "from collections import defaultdict, Counter \n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "tokenizer = nltk.tokenize.regexp.WordPunctTokenizer()\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stopwords = set(stopwords.words('english'))\n",
    "\n",
    "def load_json(file):\n",
    "    with open(file, 'r') as f: data = json.load(f)\n",
    "    return data \n",
    "\n",
    "''' \n",
    "PROCESS SENTENCES \n",
    "for each doc, stokenize terms, remove stop words, non alpha characters, \n",
    "stem and lemmatize each term\n",
    "'''\n",
    "def lemmatize(word):\n",
    "    # create an object of WordNetLemmatizer\n",
    "    lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "    lemma      = lemmatizer.lemmatize(word, 'v')\n",
    "    if lemma  == word: lemma = lemmatizer.lemmatize(word, 'n')\n",
    "    return lemma\n",
    "\n",
    "def lemmatize_document(document):\n",
    "    lemmatized_document = []\n",
    "    for word in document:\n",
    "        if word.isalnum():\n",
    "            lemmatized_document.append(lemmatize(word.lower()))\n",
    "    return lemmatized_document\n",
    "\n",
    "def remove_non_letters(doc):\n",
    "    ''' Process the tweet text to remove unwanted symbols and characters '''\n",
    "    patterns = r'[^a-zA-Z0-9\\-%]+'\n",
    "    stripped = re.sub(patterns, ' ', doc)\n",
    "    stripped = stripped.strip()\n",
    "    return stripped\n",
    "\n",
    "def process_token(token):\n",
    "    token = token.lower()\n",
    "    token = stemmer.stem(token)\n",
    "    token = lemmatize(token)\n",
    "    return token \n",
    "\n",
    "def extract_terms(doc):\n",
    "    doc   = remove_non_letters(doc)\n",
    "    # tokenize the sentence \n",
    "    tokens = word_tokenize(doc)\n",
    "    # remove all stop words \n",
    "    tokens = [process_token(i) for i in tokens if i not in stopwords]\n",
    "    return tokens \n",
    "\n",
    "def get_term_frequencies(documents):\n",
    "    ''' Get terms' occurences in each sentence in a collection '''\n",
    "    tf = defaultdict(dict)\n",
    "    total_docs  = len(documents)\n",
    "    for doc in documents:\n",
    "        doc_id = documents.index(doc)\n",
    "        doc = extract_terms(doc)\n",
    "        for term in doc:\n",
    "            tf[term][doc_id] = tf[term].get(doc_id, 0) + 1 \n",
    "    return tf, total_docs\n",
    "\n",
    "def get_tfidf(tf, total_docs):\n",
    "    ''' \n",
    "    Calculate and return TF*IDF for all terms in a collection \n",
    "    IDF = log(N/df)\n",
    "    total_docs = total number of documents in the collection\n",
    "    '''\n",
    "    tfidf = defaultdict(dict)\n",
    "    for term, doc_list in tf.items():\n",
    "        df = len(doc_list)\n",
    "        for doc_id, freq in doc_list.items(): \n",
    "            tfidf[term][doc_id] = float(tf[term][doc_id]) * log(total_docs / df)\n",
    "    return tfidf\n",
    "\n",
    "def retrieve_sentence(tfidf, query):\n",
    "    '''retrieve a sorted list of documents \n",
    "    on the order of closest distance to query'''\n",
    "    scores = {}\n",
    "    terms = extract_terms(query)\n",
    "    #print(\"Terms: \", terms, '\\n')\n",
    "    for term in terms:\n",
    "        if term in tfidf:\n",
    "            posting_list = tfidf[term]\n",
    "            for doc_id, weight in posting_list.items():\n",
    "                scores[doc_id] = scores.get(doc_id, 0) + weight\n",
    "    scores = [(k, v) for k, v in scores.items()]\n",
    "    scores = sorted(scores, key=lambda t:t[1], reverse=True)\n",
    "    return scores\n",
    "\n",
    "def get_okapibm25(tf, total_docs, documents):\n",
    "    '''Calculate and return term weights based on okapibm25'''\n",
    "    k1, b, k3 = 1.5, 0.5, 0\n",
    "    okapibm25 = defaultdict(dict)\n",
    "\n",
    "    # calculate average doc length \n",
    "    total = 0\n",
    "    for d in documents:\n",
    "        total += len(d)\n",
    "    avg_doc_length = total/len(documents)*1.0\n",
    "\n",
    "    for term, doc_list in tf.items():\n",
    "        df = len(doc_list)\n",
    "        for doc_id, freq in doc_list.items():\n",
    "            # term occurences in query\n",
    "            # qtf = question.count(term) # SEPCIAL \n",
    "            qtf = 1.2\n",
    "            idf = log((total_docs-df+0.5) / df+0.5)\n",
    "            tf_Dt = ((k1+1)*tf[term][doc_id]) / (k1*((1-b)+b*(len(documents[doc_id])/avg_doc_length) + tf[term][doc_id]))\n",
    "            if qtf == 0:\n",
    "                third = 0\n",
    "            else:\n",
    "                third = ((k3+1)*qtf) / (k3+qtf)\n",
    "                okapibm25[term][doc_id] = idf*tf_Dt*third\n",
    "\n",
    "    return okapibm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.tag import StanfordNERTagger\n",
    "from nltk.tag.stanford import StanfordPOSTagger\n",
    "from nltk import word_tokenize\n",
    "from nltk import sent_tokenize\n",
    "import timeit\n",
    "import time\n",
    "\n",
    "jar = './stanford-ner.jar'\n",
    "model = './english.all.3class.distsim.crf.ser.gz'\n",
    "st = StanfordNERTagger(model, jar)\n",
    "\n",
    "def check_uppercase(word):\n",
    "    if word[0].isupper():\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "def entity_recognize(sentences):\n",
    "       \n",
    "    result = []\n",
    "    for j, s in enumerate(st.tag_sents([word_tokenize(sent.replace('/','')) for sent in sentences])):\n",
    "        r = []\n",
    "        for i, (word,tag) in enumerate(s):\n",
    "            if i != 0:\n",
    "                if check_uppercase(word) and tag == 'O':\n",
    "                    r.append((word, 'OTHER'))\n",
    "                elif word.isdigit():\n",
    "                    r.append((word, 'NUMBER'))\n",
    "                elif tag == 'ORGANIZATION':\n",
    "                    r.append((word, 'OTHER'))\n",
    "                else:\n",
    "                    r.append((word, tag))\n",
    "            else:\n",
    "                if word.isdigit():\n",
    "                    r.append((word, 'NUMBER'))\n",
    "                elif tag == 'ORGANIZATION':\n",
    "                    r.append((word, 'OTHER'))\n",
    "                else:\n",
    "                    r.append((word, tag))\n",
    "        result.append(r)\n",
    "    return result\n",
    "\n",
    "def rechunk(ner_output):\n",
    "    tag, prev_tag = '',None\n",
    "    clean_chunked = []\n",
    "    for j, s in enumerate(ner_output):\n",
    "        chunked,tag, prev_tag = [],'',None\n",
    "        for i, word_tag in enumerate(s):\n",
    "            word, tag = word_tag\n",
    "            if tag in ['PERSON','OTHER','LOCATION','NUMBER'] and tag == prev_tag:\n",
    "                chunked[-1] += word_tag\n",
    "            else:\n",
    "                chunked.append(word_tag)\n",
    "            prev_tag = tag\n",
    "       \n",
    "        clean_chunked.append([tuple([' '.join(wordtag[::2]), wordtag[-1]])\n",
    "            if len(wordtag) != 2 else wordtag for wordtag in chunked])\n",
    "    return clean_chunked\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "import timeit\n",
    "import operator\n",
    "from itertools import groupby\n",
    "import math\n",
    "\n",
    "\n",
    "rules = [\n",
    "            ('person','PERSON'),\n",
    "            ('location', 'LOCATION'),            \n",
    "            ('who', 'PERSON'),\n",
    "            ('why', 'OTHER'),\n",
    "            ('are', 'OTHER'),\n",
    "            ('from', 'LOCATION'),\n",
    "            ('country', 'LOCATION'),\n",
    "            ('capital', 'LOCATION'),\n",
    "            ('city', 'LOCATION'),\n",
    "            ('where', 'LOCATION'),\n",
    "            ('when', 'NUMBER'),\n",
    "            ('many', 'NUMBER'),\n",
    "            ('long','NUMBER'),\n",
    "            ('high', 'NUMBER'),\n",
    "            ('year', 'NUMBER'),\n",
    "            ('decade', 'NUMBER'),\n",
    "            ('time', 'NUMBER'),\n",
    "            ('cost', 'NUMBER'),\n",
    "            ('population', 'NUMBER'),\n",
    "            ('number','NUMBER')\n",
    "        ]\n",
    "  \n",
    "\n",
    "def detect_answer_type(question):\n",
    "    tag_of_answer = None\n",
    "    for rule in rules:\n",
    "        word = rule[0]\n",
    "        tag = rule[1]\n",
    "        if word in question.lower():\n",
    "            tag_of_answer = tag\n",
    "    if tag_of_answer == None:\n",
    "        tag_of_answer = 'OTHER'\n",
    "    return tag_of_answer\n",
    "    \n",
    "    \n",
    "def check_word_in_query(word_tag, lemmatized_query):\n",
    "    word = word_tag[0]\n",
    "    if word in lemmatized_query:\n",
    "        return True\n",
    "    return False    \n",
    "\n",
    "def get_open_class_word(query):\n",
    "    tagged = nltk.pos_tag(word_tokenize(query), tagset=\"universal\")\n",
    "    #return [p[0] for p in tagged if p[1] in [\"ADJ\", \"ADV\",\"NOUN\", \"PROPN\",\"VERB\"]]\n",
    "    # take all words that is not stop words\n",
    "    return [p[0] for p in tagged if p[0] not in stopwords]\n",
    "\n",
    "def first_pass(tagged_sent, query, sentence):\n",
    "        '''\n",
    "        calculate and append total distance between words and mutual open-class words\n",
    "        '''\n",
    "        result = []\n",
    "        keywords = get_open_class_word(query)\n",
    "        tokenized_sentence = word_tokenize(sentence.replace('/',''))\n",
    "        lemmatized_keywords = lemmatize_document(keywords)\n",
    "        lemmatized_sentence = lemmatize_document(word_tokenize(sentence))\n",
    "        \n",
    "        index_of_keywords = []\n",
    "        word_in_sentence = []\n",
    "        \n",
    "        for lemmatized_keyword in lemmatized_keywords:\n",
    "            if lemmatized_keyword in lemmatized_sentence:\n",
    "                word_in_sentence.append(lemmatized_keyword)\n",
    "                \n",
    "        for word in word_in_sentence:\n",
    "            index_of_keyword = lemmatized_sentence.index(word)\n",
    "            index_of_keywords.append(index_of_keyword)\n",
    "            \n",
    "\n",
    "        for word_tag in tagged_sent:\n",
    "            try:\n",
    "                index_of_word = tokenized_sentence.index(word_tag[0])\n",
    "            except ValueError:\n",
    "                # error because word_tag[0] is actually a phrasal noun, take index of first word\n",
    "                first_word = word_tokenize(word_tag[0])[0]\n",
    "                index_of_word = tokenized_sentence.index(first_word)\n",
    "                    \n",
    "            total_distance = 0\n",
    "            for index in index_of_keywords:\n",
    "                total_distance += abs(index_of_word - index)\n",
    "\n",
    "            # add total distance to tuple\n",
    "            entity = word_tag + (total_distance,)\n",
    "            # filter out words with meaningless character and total distance = 0\n",
    "            if (entity[2] != 0) and (entity[0] != ',') and (entity[0] not in stopwords) and (entity[0] not in [':',',','(',')','.','-','[', ']','``','\"\"','%',\"'\"]):\n",
    "                entity1 = re.sub(' \\)','',entity[0])\n",
    "                entity2 = re.sub(' \\(','',entity1)\n",
    "                entity3 = re.sub(':','',entity2)\n",
    "                result.append((entity3, word_tag[1], total_distance))\n",
    "\n",
    "        return result\n",
    "\n",
    "def second_pass(first_pass_result, query):\n",
    "        # get answer type\n",
    "        tag = detect_answer_type(query)\n",
    "        result = []\n",
    "        high_ranked = []\n",
    "        low_ranked = []\n",
    "        # filter out entity with tag pertain to answer type, rank higher than those do not match answer type\n",
    "        for entry in first_pass_result:            \n",
    "            if entry[1] == tag:\n",
    "                high_ranked.append(entry)\n",
    "            else:\n",
    "                low_ranked.append(entry)\n",
    "                      \n",
    "        result.extend(high_ranked)\n",
    "        result.extend(low_ranked)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "def third_pass(question, second_pass_result):\n",
    "\n",
    "    third_pass_result = []\n",
    "    lemmatized_query = lemmatize_document(word_tokenize(question))\n",
    "    # group entities with same tag\n",
    "    for key, group in groupby(second_pass_result, key = operator.itemgetter(1)):\n",
    "        \n",
    "        # sort entities by distance\n",
    "        group_sorted = sorted(group, key = operator.itemgetter(2), reverse = False)\n",
    "                \n",
    "        # sort entities again by putting entity occur in question to lowest of group\n",
    "        high_ranked = []\n",
    "        low_ranked = []\n",
    "        result = []        \n",
    "        for entity in group_sorted:\n",
    "            if check_word_in_query(entity, lemmatized_query):\n",
    "                low_ranked.append(entity)\n",
    "            else:\n",
    "                high_ranked.append(entity)                \n",
    "        result.extend(high_ranked)\n",
    "        result.extend(low_ranked)\n",
    "        \n",
    "        \n",
    "        third_pass_result.extend(result)\n",
    "    \n",
    "    return third_pass_result[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "def print_csv():\n",
    "    data = load_json('QA_test.json')\n",
    "\n",
    "    csv_file = open('output.csv', 'w', encoding='utf-8')\n",
    "    writer = csv.writer(csv_file)\n",
    "    writer.writerow(['id', 'answer'])\n",
    "    \n",
    "    for article in data:\n",
    "        collection = article['sentences']\n",
    "        tf, total_docs = get_term_frequencies(collection)\n",
    "        #tfidf = get_tfidf(tf, total_docs)\n",
    "        tagged_sentences = rechunk(entity_recognize(collection))       \n",
    "        for qa in article['qa']:\n",
    "            question = qa['question']\n",
    "            qa_id = qa['id']\n",
    "            okapibm25 = get_okapibm25(tf, total_docs, collection)\n",
    "            sentence_id = retrieve_sentence(okapibm25, question)\n",
    "            if len(sentence_id) == 0:\n",
    "                writer.writerow([qa_id, 'Not sure'])\n",
    "            else:\n",
    "                doc_id = sentence_id[0][0]\n",
    "                first_pass_result = first_pass(tagged_sentences[doc_id], question, collection[doc_id])\n",
    "                if first_pass_result == []:\n",
    "                    writer.writerow([qa_id, 'Not sure'])\n",
    "                else:\n",
    "                    second_pass_result = second_pass(first_pass_result, question)\n",
    "                    third_pass_result = third_pass(question, second_pass_result)\n",
    "                    writer.writerow([qa_id, third_pass_result])\n",
    "\n",
    "    csv_file.close()\n",
    "\n",
    "print_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
